{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../sentence-sentimental')\n",
    "\n",
    "from dataset.datasets import SentimentalDataset\n",
    "from metrics.metrics import compute_metrics\n",
    "\n",
    "from sklearn.datasets import load_iris # 샘플 데이터 로딩\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 443M/443M [00:04<00:00, 100MB/s]  \n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_NAME = 'klue/roberta-base'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/opt/ml/finance_sentiment_corpus/finance_data.csv')\n",
    "data['labels'] = data['labels'].map({'negative':0, 'neutral':1, 'positive':2})\n",
    "\n",
    "train_encoding = tokenizer(\n",
    "    data['kor_sentence'].tolist(),\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True\n",
    "    )\n",
    "\n",
    "train_set = SentimentalDataset(train_encoding, data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4846 4846\n"
     ]
    }
   ],
   "source": [
    "print(len(data['kor_sentence']), len(data['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_train, sentence_test, label_train, label_test = train_test_split(data['kor_sentence'], data['labels'],\n",
    "                                                                            test_size=0.2, \n",
    "                                                                            shuffle=True, stratify=data['labels'], # label에 비율을 맞춰서 분리\n",
    "                                                                            random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoding = tokenizer(\n",
    "    sentence_train.tolist(),\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True\n",
    "    )\n",
    "\n",
    "test_encoding = tokenizer(\n",
    "    sentence_test.tolist(),\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SentimentalDataset(train_encoding, label_train)\n",
    "test_set = SentimentalDataset(test_encoding, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentimentalDataset' object has no attribute 'len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/datasets.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/datasets.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_set\u001b[39m.\u001b[39;49mlen()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SentimentalDataset' object has no attribute 'len'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1828                핀란드 식품업계 기업들은 우크라이나가 제공하는 기회에 관심이 많다.\n",
       "4809    펜틱+아이넨은 미디어하우스가 제공하는 인터넷 콘텐츠 대부분이 영원히 무료일 수는 없...\n",
       "1166                    또한 이러한 시장에 대한 6년간의 역사적 분석이 제공됩니다.\n",
       "277     루키 총리는 핀란드 사본리나에 위치한 키론살미 해협을 가로지르는 철제 구조물의 인도...\n",
       "4670    2009년 8월 3일 핀란드 미디어 그룹 Ilkka-Yhtyma Oyj(헬: ILK...\n",
       "                              ...                        \n",
       "3515    알스트롬 주식거래소 7.2.2.27 10시 30분에 발표 알스트롬 주식회사의 총 5...\n",
       "4503           라우트 대변인은 \"다른 인력에 대한 적응 조치는 당분간 적절하다\"고 말했다.\n",
       "434     주식수 증가는 2006년 스톡옵션제도에 따라 회사 경영에 부여된 주식선택권에 따른 ...\n",
       "3212         개발사업단도 지난해 11월 대비 개발사업단가를 3분의 1가량 낮추겠다고 밝혔다.\n",
       "3773                    이 건물에는 제품 개발 및 테스트 실험실이 들어설 예정이다.\n",
       "Name: kor_sentence, Length: 3876, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset.datasets.SentimentalDataset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6433bde22504cbf34326cab27df20b94e196fcf98213f776ce9807cc95ec7583"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
