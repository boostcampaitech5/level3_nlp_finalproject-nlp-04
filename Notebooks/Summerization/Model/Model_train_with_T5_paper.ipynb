{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4d6f4e-9af2-4d80-83ca-a704b7839e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79486004-f07e-4cd1-a061-7edd26b6168b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee82085818a14b6791f65ef00e86eed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '안녕하세요, ^^*오늘은 제가 좋아하는 맛집을 소개해드리려고 해요.바로바로~~~~~~'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import pipeline\n",
    "# generator = pipeline('text-generation', model = 'EleutherAI/polyglot-ko-5.8b', device=\"cuda\")\n",
    "# generator(\"안녕하세요, \", max_length = 30, num_return_sequences=1)\n",
    "# ## [{'generated_text': \"Hello, I'm a language modeler. So while writing this, when I went out to meet my wife or come home she told me that my\"},\n",
    "# ##  {'generated_text': \"Hello, I'm a language modeler. I write and maintain software in Python. I love to code, and that includes coding things that require writing\"}, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a0107-d940-43b0-9ed3-1d12abda7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator(\"사람에게 물이 부족하면\", max_length = 120, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1308f288-df02-4bf8-9b2b-dc41190d1302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "import evaluate\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5466c5a3-2c2e-4049-968d-ce7196f66f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# str_model = 'KETI-AIR/ke-t5-base'\n",
    "str_model =\"paust/pko-t5-base\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(str_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(str_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2166f98-b6a5-4d2a-bad2-7e49b06255a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['doc_id', 'title', 'ipc', 'all', 'sum'],\n",
       "        num_rows: 287982\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['doc_id', 'title', 'ipc', 'all', 'sum'],\n",
       "        num_rows: 578\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dataset = \"./dataset/paper/paper_dataset\"\n",
    "\n",
    "dataset_paper = load_from_disk(path_dataset)\n",
    "dataset_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa05541-97a8-4687-bd42-24da55d79fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"요약: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"all\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"sum\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0167b23f-e78f-4410-a8b8-69c539a63345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /opt/ml/dataset/paper/paper_dataset/train/cache-379987f59dadf2bb.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffd157f688943fbb47f625d5e23d9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_paper_tokenized = dataset_paper.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16e8cadd-098e-476e-8037-38dd1147083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전문\n",
      "2012년 경상남도 진주시 농산물도매시장에 판매중인 감귤에서 분홍빛열매썩음병이 발생하였다. 병징은 감귤 과실 표면이 수침상으로 물러지고 썩으면서 그 위에 분홍빛 곰팡이가 많이 형성되었다. 균총의 색깔은 처음에 흰색이고 배양기간이 경과됨에 따라 배지 표면에 분홍빛의분생포자가 많이 형성되었다. 균사생육 적온은 25oC이었다. 분생포자의 모양은 서양배형이며 좌우 zigzag로 부착하며 성숙한 분생포자는 2세포로 되어 있으며 크기는 12?26 × 8?12 μm이었다. 분생자경은 균사표면에 직립으로 형성하고, 폭이 4-5 μm이고 무색이었다. rDNA의 completeinternal transcribed spacer(ITS) 영역의 염기서열을 분석하였고, 분석된 염기서열(613 bp)을 BLASTN 프로그램으로확인한 결과, Trichothecium roseum와 99%의 상동성을 나타내었다. 이와 같이 감귤에서 발생한 병징과 병원균의균학적 특징을 기초로 하여 이 병을 Trichothecium roseum(Pers.) Link ex Gray에 의한 감귤 분홍빛열매썩음병으로명명하고자 제안한다.\n",
      "요약\n",
      "2012년 경상남도 진주시 농산물도매시장에 판매중인 감귤에서 분홍빛열매썩음병이 발생했다. 분생포자의 모양은 서양배형이며 좌우 zigzag로 부착하며 성숙한 분생포자는 2세포로 되어 있으며 크기는 12?26 × 8?12 μm이었다.\n",
      "\n",
      "2012년 경상남도 진주시 농산물도매시장에 판매중인 감귤에서 분홍빛열매썩음병이 발생했다. 분생포자의 모양은 서양배형이며 좌우 zigzag로 부착하며 성숙한 분생포자는 2세포로 되어 있으며 크기는 12?26 × 8?12 μm이었다.</s>\n"
     ]
    }
   ],
   "source": [
    "print(\"전문\")\n",
    "print(dataset_paper_tokenized[\"train\"][100][\"all\"])\n",
    "print(\"요약\")\n",
    "print(dataset_paper_tokenized[\"train\"][100][\"sum\"])\n",
    "print()\n",
    "print(tokenizer.decode(dataset_paper_tokenized[\"train\"][100][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41d2c757-6890-4284-8a40-5bde4520f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df7358a6-807f-460a-8ab6-2849b4ea2dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    print(predictions, labels)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4bd4aea-b113-4a5d-b69b-959868bb4e3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128073' max='143992' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128073/143992 15:30:10 < 1:55:37, 2.29 it/s, Epoch 3.56/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.321900</td>\n",
       "      <td>0.312891</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.279300</td>\n",
       "      <td>0.293201</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>0.097200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.279073</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.100100</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.269900</td>\n",
       "      <td>0.271338</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.098800</td>\n",
       "      <td>0.099300</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.262079</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.100700</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.264940</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.100700</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.255469</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.248780</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.100100</td>\n",
       "      <td>0.100100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>0.255119</td>\n",
       "      <td>0.100100</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.100500</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.242868</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.252300</td>\n",
       "      <td>0.245105</td>\n",
       "      <td>0.099200</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.099300</td>\n",
       "      <td>0.099600</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.240337</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.106500</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.243100</td>\n",
       "      <td>0.239554</td>\n",
       "      <td>0.103700</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.104100</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>0.243725</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.236655</td>\n",
       "      <td>0.098700</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.098800</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>0.240167</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>0.234569</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.104800</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.247600</td>\n",
       "      <td>0.230813</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.228300</td>\n",
       "      <td>0.236453</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.227800</td>\n",
       "      <td>0.231006</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.225200</td>\n",
       "      <td>0.230552</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.217300</td>\n",
       "      <td>0.229204</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.214900</td>\n",
       "      <td>0.226719</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.224300</td>\n",
       "      <td>0.223870</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.223686</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.217900</td>\n",
       "      <td>0.227150</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.108600</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.213000</td>\n",
       "      <td>0.222517</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.226600</td>\n",
       "      <td>0.221138</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.219500</td>\n",
       "      <td>0.222767</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.104100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.222700</td>\n",
       "      <td>0.220268</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.107300</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>0.222766</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.211000</td>\n",
       "      <td>0.219519</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>0.220934</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>0.220761</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.104100</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.220064</td>\n",
       "      <td>0.101800</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>0.102500</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>0.219950</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.218738</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.204300</td>\n",
       "      <td>0.218193</td>\n",
       "      <td>0.106200</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>0.106800</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>0.214384</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.204900</td>\n",
       "      <td>0.218057</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>0.106200</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.205300</td>\n",
       "      <td>0.212430</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.204500</td>\n",
       "      <td>0.213373</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.103700</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>0.211706</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.103700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.211800</td>\n",
       "      <td>0.215255</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.198900</td>\n",
       "      <td>0.211147</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.202600</td>\n",
       "      <td>0.211945</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.197100</td>\n",
       "      <td>0.213772</td>\n",
       "      <td>0.101400</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.203300</td>\n",
       "      <td>0.212218</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.104800</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.205800</td>\n",
       "      <td>0.211518</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.103700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.203600</td>\n",
       "      <td>0.211463</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.202300</td>\n",
       "      <td>0.210687</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.106400</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>18.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.210914</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.201900</td>\n",
       "      <td>0.210024</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.202200</td>\n",
       "      <td>0.209621</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.106400</td>\n",
       "      <td>0.107200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>0.209140</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.193600</td>\n",
       "      <td>0.210531</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.189400</td>\n",
       "      <td>0.209950</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.193100</td>\n",
       "      <td>0.208772</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.210169</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.194200</td>\n",
       "      <td>0.208823</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.190400</td>\n",
       "      <td>0.207102</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.106200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.204800</td>\n",
       "      <td>0.208545</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>0.106200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.209584</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>0.206511</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  262  222 ... 6596  476  222]\n",
      " [   0  262  222 ... 1838  389  222]\n",
      " [   0 7717  222 ... 1018 1024  222]\n",
      " ...\n",
      " [   0 2739  222 ...  222 1440  333]\n",
      " [   0 1373  222 ...  222 8589  222]\n",
      " [   0 4376  222 ...  248 8497  302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   262   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n",
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0   262   222 ...  6596   476   222]\n",
      " [    0   582   222 ...  1838   389   222]\n",
      " [    0  7717   222 ...  1018  1024   222]\n",
      " ...\n",
      " [    0   262   222 ...  1001  2452   279]\n",
      " [    0 43327   972 ...  1085   222  1630]\n",
      " [    0  4376   222 ...   248  8497   302]] [[ 262  222 1271 ...  681  222    1]\n",
      " [ 262  222 1271 ...  222  517    1]\n",
      " [7717  222 1164 ... -100 -100 -100]\n",
      " ...\n",
      " [ 262  222 1271 ... -100 -100 -100]\n",
      " [ 262  222 6851 ... -100 -100 -100]\n",
      " [4376  222 8796 ...  222  657    1]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m\n\u001b[1;32m      5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     22\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     23\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m model(dataset_paper_tokenized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1532\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1534\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1536\u001b[0m )\n\u001b[0;32m-> 1537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1802\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1799\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1801\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1802\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1805\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1807\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1808\u001b[0m ):\n\u001b[1;32m   1809\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1810\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:2647\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2646\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2647\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2650\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:2672\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2671\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2672\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2674\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/accelerate/utils/operations.py:553\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/accelerate/utils/operations.py:541\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/accelerate/utils/operations.py:553\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/accelerate/utils/operations.py:541\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: ConvertOutputsToFp32.__call__ at line 541 (2 times), autocast_decorator.<locals>.decorate_autocast at line 14 (2 times), convert_outputs_to_fp32.<locals>.forward at line 553 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/accelerate/utils/operations.py:553\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/accelerate/utils/operations.py:541\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1717\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1714\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1717\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1732\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1094\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1082\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1083\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m     )\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1094\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:724\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 724\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:635\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    624\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    633\u001b[0m ):\n\u001b[1;32m    634\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 635\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    647\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:560\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m     position_bias_masked \u001b[38;5;241m=\u001b[39m position_bias\n\u001b[0;32m--> 560\u001b[0m scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_bias_masked\n\u001b[1;32m    561\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(scores\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(\n\u001b[1;32m    562\u001b[0m     scores\n\u001b[1;32m    563\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    564\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[1;32m    565\u001b[0m     attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    566\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=str_model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"summary\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    # gradient_accumulation_steps=2,\n",
    "    # eval_accumulation_steps=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    eval_steps=2000,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_paper_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_paper_tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model(dataset_paper_tokenized[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c703c4d5-a377-40d2-92fd-c9447f3d3ff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model(tokenizer(\"우아아아아아\", max_length=1024, truncation=True, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee9ab2-1326-476f-a472-23d1794c9410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "accelerate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43edf680-b4dc-499a-b4ae-c7b69cae8609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c6c6833f-fd44-4e48-8ded-64720d223930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': '빅테크 기업이 아닌 일반 기업이나 연구자가 거대 언어'}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"./summary/checkpoint-128000\")\n",
    "# summarizer = pipeline(\"text2text-generation\", model=\"./summary/checkpoint-128000\")\n",
    "summarizer(\"요약: \" + \"GPT-3를 필두로 하여 거대 파라미터의 모델에 대규모 코퍼스를 학습한 (초)거대 언어모델은 자연스러운 문장을 생성하거나 다양한 태스크를 적은 학습 데이터로 수행하는 등 뛰어난 가능성을 보였다. 하지만 학습에 막대한 자본이 필요한 거대 언어모델은 AI의 독점화 및 권력화, 그리고 데이터 및 컴퓨팅 파워 격차에 따른 기술 격차 심화를 낳을 것이라는 우려도 존재한다. 빅테크 기업이 아닌 일반 기업이나 연구자가 거대 언어 모델을 다루기는 쉽지 않은 것은 엄연한 사실이다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "13e7d548-91d4-4783-980a-475db86d42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_news = \"\"\"\n",
    "뚜렷한 이유는 안밝혀..지난해 사업협력 단순투자로 공시했는데네이버 카페 주주모임, 불매운동하자는 글까지3명 사외이사 재선임 최종 부결돼도상법상 임시주총까지 퇴임이사로 활동 가능[이데일리 김현아 기자]KT의 지분 7.79%(현대자동차 4.69%, 현대모비스 3.1%)를 보유한 현대자동차그룹이 오는 31일 KT 주주총회에서 KT 사외이사 3명의 재선임안(임기 1년)에 대해 반대하겠다는 뜻을 KT에 전달한 것으로 확인되자, KT 개인주주들이 분노하고 있다. 현대차그룹은 강충구 고려대 전기전자공학부 교수, 여은정 중앙대 경영학부 교수, 표현명  한국타이어테크놀로지 사외이사 등 3명에 대해 반대의 뜻을 KT에 전했다. 앞서 현대차그룹은 윤경림 차기 CEO 후보에 대해서도 “이사회가 대주주 의견을 고려해야 한다”는 취지를 밝혀, 사실상 최대 주주인 국민연금과 뜻을 함께 하겠다는 의지를 밝힌 바 있다. 지난해 KT와 현대차는 7500억 규모의 자사주 맞교환을 통해 도심항공모빌리티(UAM) 등 미래 모빌리티 분야의 경쟁력을 키우기로 합의한 바 있다. 현대차 그룹이 KT 지분 4.6%, 현대모비스가 KT 지분 3.1%를, KT가 현대차 지분 1.04%와 현대모비스 지분 1.46%를 갖게 됐다.\n",
    "\"\"\"\n",
    "\n",
    "def get_summary(full_news):\n",
    "    news_tokenized = len(tokenizer(full_news)[\"input_ids\"])\n",
    "    print(news_tokenized)\n",
    "    \n",
    "    return summarizer(full_news, min_length=news_tokenized // 2, max_length=news_tokenized // 2 + (news_tokenized // 4))\n",
    "\n",
    "# 1934\n",
    "\n",
    "# [{'generated_text': '제주항공에 자금을 지원하기 위해 제주항공 주식을 담보로 돈을 빌리는 AK홀딩스, SK케미칼 주식 공개매수 나서는 SK디스커버리, 하나금융14호스팩으로 본 스팩의 상장폐지 절차와 관련한 이야기 등을 담아봤어요.AK홀딩스 제주항공 유증 참여 위해 교환사채 발행애경그룹 지주회사AK홀딩스가 자회사인 저비용항공사제주항공의 주주배정 유상증자 참여를 위해 1300억원 )                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 '}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3c03de28-85d6-42e8-b246-4fe55d8782e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n",
      "[{'summary_text': 'KT의 지분 7.79%를 보유한 현대자동차그룹이 오는 31일 KT 주주총회에서 KT 사외이사 3명의 재선임안에 대해 반대하겠다는 뜻을 KT에 전달한 것으로 확인되자, KT 개인주주들이 분노하고 있다. 현대차그룹은 강충구 고려대 전기전자공학부 교수, 여은정 중앙대 경영학부 교수, 표현명 한국타이어테크놀로지 사외이사 등 3명에 대해                                                                                        '}]\n",
      "254\n",
      "[{'summary_text': '네이버 KT주주모임 카페에서 아이디 알바트로스님은 “현대차가 선을 넘는건가요?”라고 했고, chsu6366님은 “현대차그룹의 양아치 경영에 분노를 표하며 주주들이 불매운동에 적극적으로 동참하시길 당부드린다. KT CFO는 현대차그룹 지분을 매각하시길 바란다”고 했다. 앞서 주주모임 카페 개설자도 이데일리와의 통화에서 �'}]\n",
      "265\n",
      "[{'summary_text': '강충구, 여은정, 표현명 이사가 주총에서 재선임되지 않으면 이사는 1명만 남게 된다. 현대차 반대해 최종 부결돼도 상법으로 이사 의무 유지하지만, 법조계에 따르면 설사 KT 이사가 한 명도 안 남아도 상법상 이사의 결원 조항으로 인해 새 이사회 구성까지 임무를 담당할 수 있다. 퇴임이사 자격으로 가능하다.\\n\\n퇴임이사 자격으로 '}]\n",
      "252\n",
      "[{'summary_text': 'KT 이사회의 최소 의결 정족 원수는 3명인데, 현대차 주장대로 선임안이 부결되면 이사는 1명 남게 된다. 하지만, 부결돼도 해당 이사들은 이 조항에 근거해 임시주총에서 새로운 이사가 추천돼 선임될 때까지 퇴임이사로 활동할 수 있다. 한편 KT의 주가는 다시 3만 원대 아래로 추락했다. 29일 현재 2만 9200원이 '}]\n"
     ]
    }
   ],
   "source": [
    "list_news = [\"뚜렷한 이유는 안밝혀..지난해 사업협력 단순투자로 공시했는데네이버 카페 주주모임, 불매운동하자는 글까지3명 사외이사 재선임 최종 부결돼도상법상 임시주총까지 퇴임이사로 활동 가능[이데일리 김현아 기자]KT의 지분 7.79%(현대자동차 4.69%, 현대모비스 3.1%)를 보유한 현대자동차그룹이 오는 31일 KT 주주총회에서 KT 사외이사 3명의 재선임안(임기 1년)에 대해 반대하겠다는 뜻을 KT에 전달한 것으로 확인되자, KT 개인주주들이 분노하고 있다. 현대차그룹은 강충구 고려대 전기전자공학부 교수, 여은정 중앙대 경영학부 교수, 표현명  한국타이어테크놀로지 사외이사 등 3명에 대해 반대의 뜻을 KT에 전했다. 앞서 현대차그룹은 윤경림 차기 CEO 후보에 대해서도 “이사회가 대주주 의견을 고려해야 한다”는 취지를 밝혀, 사실상 최대 주주인 국민연금과 뜻을 함께 하겠다는 의지를 밝힌 바 있다. 지난해 KT와 현대차는 7500억 규모의 자사주 맞교환을 통해 도심항공모빌리티(UAM) 등 미래 모빌리티 분야의 경쟁력을 키우기로 합의한 바 있다. 현대차 그룹이 KT 지분 4.6%, 현대모비스가 KT 지분 3.1%를, KT가 현대차 지분 1.04%와 현대모비스 지분 1.46%를 갖게 됐다.\",\n",
    "\"당시 양사는 “사업협력을 위한 단순투자”라면서 지분투자 목적을 ‘단순투자’로 공시했다. 하지만, 이 같은 보도가 나오자 개인주주들은 반발하고 있다. 네이버 KT주주모임 카페에서 아이디 알바트로스님은 “현대차가 선을 넘는건가요?”라고 했고, chsu6366님은 “현대차그룹의 양아치 경영에 분노를 표하며 주주들이 불매운동에 적극적으로 동참하시길 당부드린다. KT CFO는 현대차그룹 지분을 매각하시길 바란다”고 했다. 앞서 주주모임 카페 개설자도 이데일리와의 통화에서 “이번 주총에서 현대차, 신한은행이 반대하면, 주주제안으로 미래사업 제휴에서 두 회사를 제외하고 KT와 상호주식교환 등을 한 걸 해지하라고 요구할 예정이다.\",\n",
    "\"카페 안에선 굉장히 좋은 현대차에 대해 불매 운동까지 하자는 글도 있다”고 밝힌 바 있다. 한편 이강철 이사에 이어 어제(28일),김대유 이사(DB생명 사외이사)와 유희열 이사(한국 이산화탄소 포집 및 처리 연구개발센터(KCRC) 이사장) 등 지난 정부 출신 사외이사들이 사퇴하면서 KT 이사회는 사내이사 0명, 사외이사 4명이 남은 상태다. 이중 강충구, 여은정, 표현명 이사가 주총에서 재선임되지 않으면 이사는 1명만 남게 된다. 현대차 반대해 최종 부결돼도 상법으로 이사 의무 유지하지만, 법조계에 따르면 설사 KT 이사가 한 명도 안 남아도 상법상 이사의 결원 조항으로 인해 새 이사회 구성까지 임무를 담당할 수 있다. 퇴임이사 자격으로 가능하다.\",\n",
    "\"ESG 자문기관인 서스틴베스트 자문 변호사에 따르면, 상법 386조 1항에 따라, 법률 또는 정관에서 정한 이사의 원수를 결한 경우에는 임기의 만료 또는 사임으로 인하여 퇴임한 이사는 새로 선임된 이사가 취임할 때까지 이사로서의 권리와 의무가 있다고 돼 있다. KT 이사회의 최소 의결 정족 원수는 3명인데, 현대차 주장대로 선임안이 부결되면 이사는 1명 남게 된다. 하지만, 부결돼도 해당 이사들은 이 조항에 근거해 임시주총에서 새로운 이사가 추천돼 선임될 때까지 퇴임이사로 활동할 수 있다. 한편 KT의 주가는 다시 3만 원대 아래로 추락했다. 29일 현재 2만 9200원이 돼 전날보다 2.83% 하락한 채 마감했다.\"]\n",
    "\n",
    "for news in list_news:\n",
    "    print(get_summary(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8a358-74c6-4a1b-9cdc-e0dd34119fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
