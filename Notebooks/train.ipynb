{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/final/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "from transformers import DebertaV2ForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "from dataset.datasets import SentimentalDataset\n",
    "from metrics.metrics import compute_metrics\n",
    "\n",
    "from sklearn.datasets import load_iris # 샘플 데이터 로딩\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.utils import config_seed\n",
    "\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "config_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 및 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32002, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'klue/roberta-large'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': ['[COMPANY]','[/COMPANY]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"team-lucid/deberta-v3-base-korean\"\n",
    "\n",
    "# model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 구성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 기사 전체 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('/opt/ml/finance_sentiment_corpus/merged_samsung_filtered.csv')\n",
    "\n",
    "# def extract_label(json_str) :\n",
    "#     data_dict = eval(json_str)  # JSON 문자열을 파이썬 딕셔너리로 변환\n",
    "#     return data_dict[\"label\"]\n",
    "\n",
    "# # \"label\" 값을 추출하여 새로운 Series 생성\n",
    "# data['labels'] = data[\"labels\"].apply(extract_label)\n",
    "# data['labels'] = data['labels'].map({'부정':0, '긍정':1})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 기사 앞뒤 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content_corpus</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>삼성전자</td>\n",
       "      <td>데이터센터 전력 40% 차지하는 D램… 삼성·SK하이닉스, ‘전성비’ ...</td>\n",
       "      <td>2023.07.10 15:29</td>\n",
       "      <td>챗GPT 시대 화두로 떠오른 전력효율성 문제 ”전력 먹는 하마, D램 전력효율성 개...</td>\n",
       "      <td>{\"label\": \"긍정\", \"reason\": \"전력효율성 개선을 위한 솔루션 개발...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>삼성전자</td>\n",
       "      <td>“삼성전자가 식품도 팔았어?”…신규 가입 일단 종료한 사연</td>\n",
       "      <td>2023.07.10 15:07</td>\n",
       "      <td>삼성 가전제품 구매고객에 삼성닷컴 내 e-식품관에서  할인혜택 주며 ‘락인’ 기대 ...</td>\n",
       "      <td>{\"label\": \"부정\", \"reason\": \"삼성전자 멤버십 플랜 신규 고객 모...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>삼성전자</td>\n",
       "      <td>SGC솔루션 '도어글라스'…삼성·LG 세탁기·건조기에 공급</td>\n",
       "      <td>2023.07.10 15:05</td>\n",
       "      <td>해외 가전 브랜드 공략…B2B 사업 확장 SGC솔루션 논산 공장.  .   생활유리...</td>\n",
       "      <td>{\"label\": \"긍정\", \"reason\": \"해외 가전 브랜드들을 공략하며 전 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>삼성전자</td>\n",
       "      <td>‘페이커’ 내세운 삼성 OLED 게이밍 모니터 글로벌 3천대 돌파</td>\n",
       "      <td>2023.07.10 14:58</td>\n",
       "      <td>북미·유럽 등 예약 판매 3000대 돌파 10일 오후 6시 삼성닷컴 ‘페이커’ 출연...</td>\n",
       "      <td>{\"label\": \"긍정\", \"reason\": \"오디세이 OLED G9의 글로벌 예...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>삼성전자</td>\n",
       "      <td>네이처 게재 등 성과…삼성휴먼테크논문대상, 30년 맞았다</td>\n",
       "      <td>2023.07.10 14:48</td>\n",
       "      <td>29년간 3만6558편 논문 접수…수상자 5312명 9월1일부터 올해 대상 접수…상...</td>\n",
       "      <td>{\"label\": \"긍정\", \"reason\": \"삼성휴먼테크논문대상이 올해로 시행 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company                                       title              date  \\\n",
       "0    삼성전자  데이터센터 전력 40% 차지하는 D램… 삼성·SK하이닉스, ‘전성비’ ...  2023.07.10 15:29   \n",
       "1    삼성전자            “삼성전자가 식품도 팔았어?”…신규 가입 일단 종료한 사연  2023.07.10 15:07   \n",
       "2    삼성전자            SGC솔루션 '도어글라스'…삼성·LG 세탁기·건조기에 공급  2023.07.10 15:05   \n",
       "3    삼성전자        ‘페이커’ 내세운 삼성 OLED 게이밍 모니터 글로벌 3천대 돌파  2023.07.10 14:58   \n",
       "4    삼성전자             네이처 게재 등 성과…삼성휴먼테크논문대상, 30년 맞았다  2023.07.10 14:48   \n",
       "\n",
       "                                      content_corpus  \\\n",
       "0  챗GPT 시대 화두로 떠오른 전력효율성 문제 ”전력 먹는 하마, D램 전력효율성 개...   \n",
       "1  삼성 가전제품 구매고객에 삼성닷컴 내 e-식품관에서  할인혜택 주며 ‘락인’ 기대 ...   \n",
       "2  해외 가전 브랜드 공략…B2B 사업 확장 SGC솔루션 논산 공장.  .   생활유리...   \n",
       "3  북미·유럽 등 예약 판매 3000대 돌파 10일 오후 6시 삼성닷컴 ‘페이커’ 출연...   \n",
       "4  29년간 3만6558편 논문 접수…수상자 5312명 9월1일부터 올해 대상 접수…상...   \n",
       "\n",
       "                                              labels  \n",
       "0  {\"label\": \"긍정\", \"reason\": \"전력효율성 개선을 위한 솔루션 개발...  \n",
       "1  {\"label\": \"부정\", \"reason\": \"삼성전자 멤버십 플랜 신규 고객 모...  \n",
       "2  {\"label\": \"긍정\", \"reason\": \"해외 가전 브랜드들을 공략하며 전 ...  \n",
       "3  {\"label\": \"긍정\", \"reason\": \"오디세이 OLED G9의 글로벌 예...  \n",
       "4  {\"label\": \"긍정\", \"reason\": \"삼성휴먼테크논문대상이 올해로 시행 ...  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/opt/ml/finance_sentiment_corpus/merged/merged_all.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3778, 5)\n",
      "(3777, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content_corpus</th>\n",
       "      <th>labels</th>\n",
       "      <th>content_corpus_company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>데이터센터 전력 40% 차지하는 D램… 삼성·SK하이닉스, ‘전성비’ ...</td>\n",
       "      <td>2023.07.10 15:29</td>\n",
       "      <td>챗GPT 시대 화두로 떠오른 전력효율성 문제 ”전력 먹는 하마, D램 전력효율성 개...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>이 기사는 [COMPANY]삼성전자[/COMPANY]에 대한 기사. [SEP]. 데...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“삼성전자가 식품도 팔았어?”…신규 가입 일단 종료한 사연</td>\n",
       "      <td>2023.07.10 15:07</td>\n",
       "      <td>삼성 가전제품 구매고객에 삼성닷컴 내 e-식품관에서  할인혜택 주며 ‘락인’ 기대 ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>이 기사는 [COMPANY]삼성전자[/COMPANY]에 대한 기사. [SEP]. “...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGC솔루션 '도어글라스'…삼성·LG 세탁기·건조기에 공급</td>\n",
       "      <td>2023.07.10 15:05</td>\n",
       "      <td>해외 가전 브랜드 공략…B2B 사업 확장 SGC솔루션 논산 공장.  .   생활유리...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>이 기사는 [COMPANY]삼성전자[/COMPANY]에 대한 기사. [SEP]. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‘페이커’ 내세운 삼성 OLED 게이밍 모니터 글로벌 3천대 돌파</td>\n",
       "      <td>2023.07.10 14:58</td>\n",
       "      <td>북미·유럽 등 예약 판매 3000대 돌파 10일 오후 6시 삼성닷컴 ‘페이커’ 출연...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>이 기사는 [COMPANY]삼성전자[/COMPANY]에 대한 기사. [SEP]. ‘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>네이처 게재 등 성과…삼성휴먼테크논문대상, 30년 맞았다</td>\n",
       "      <td>2023.07.10 14:48</td>\n",
       "      <td>29년간 3만6558편 논문 접수…수상자 5312명 9월1일부터 올해 대상 접수…상...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>이 기사는 [COMPANY]삼성전자[/COMPANY]에 대한 기사. [SEP]. 네...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        title              date  \\\n",
       "0  데이터센터 전력 40% 차지하는 D램… 삼성·SK하이닉스, ‘전성비’ ...  2023.07.10 15:29   \n",
       "1            “삼성전자가 식품도 팔았어?”…신규 가입 일단 종료한 사연  2023.07.10 15:07   \n",
       "2            SGC솔루션 '도어글라스'…삼성·LG 세탁기·건조기에 공급  2023.07.10 15:05   \n",
       "3        ‘페이커’ 내세운 삼성 OLED 게이밍 모니터 글로벌 3천대 돌파  2023.07.10 14:58   \n",
       "4             네이처 게재 등 성과…삼성휴먼테크논문대상, 30년 맞았다  2023.07.10 14:48   \n",
       "\n",
       "                                      content_corpus  labels  \\\n",
       "0  챗GPT 시대 화두로 떠오른 전력효율성 문제 ”전력 먹는 하마, D램 전력효율성 개...     1.0   \n",
       "1  삼성 가전제품 구매고객에 삼성닷컴 내 e-식품관에서  할인혜택 주며 ‘락인’ 기대 ...     0.0   \n",
       "2  해외 가전 브랜드 공략…B2B 사업 확장 SGC솔루션 논산 공장.  .   생활유리...     1.0   \n",
       "3  북미·유럽 등 예약 판매 3000대 돌파 10일 오후 6시 삼성닷컴 ‘페이커’ 출연...     1.0   \n",
       "4  29년간 3만6558편 논문 접수…수상자 5312명 9월1일부터 올해 대상 접수…상...     1.0   \n",
       "\n",
       "                              content_corpus_company  \n",
       "0  이 기사는 [COMPANY]삼성전자[/COMPANY]에 대한 기사. [SEP]. 데...  \n",
       "1  이 기사는 [COMPANY]삼성전자[/COMPANY]에 대한 기사. [SEP]. “...  \n",
       "2  이 기사는 [COMPANY]삼성전자[/COMPANY]에 대한 기사. [SEP]. S...  \n",
       "3  이 기사는 [COMPANY]삼성전자[/COMPANY]에 대한 기사. [SEP]. ‘...  \n",
       "4  이 기사는 [COMPANY]삼성전자[/COMPANY]에 대한 기사. [SEP]. 네...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/opt/ml/finance_sentiment_corpus/merged/merged_all.csv\")\n",
    "# data = pd.read_csv(\"/opt/ml/finance_sentiment_corpus/merged/merged_NAVER.csv\")\n",
    "\n",
    "def remove_idx_row(data) : \n",
    "    patterns = [r'idx\\s*:?\\s*.+?', r'라벨링\\s*:?\\s*.+?']\n",
    "    \n",
    "    for pattern in patterns :\n",
    "        mask = data['labels'].str.match(pattern)\n",
    "        data = data.drop(data[mask].index)\n",
    "\n",
    "    return data\n",
    "    \n",
    "# title과 content_corpus에서 원하는 문장 추출\n",
    "def extract_sentences(text):\n",
    "    sentences = text.split('. ')\n",
    "    if len(sentences) >= 5 :\n",
    "        return '. '.join([sentences[0], sentences[1], sentences[-2], sentences[-1]])\n",
    "    else :\n",
    "        return '. '+text\n",
    "    \n",
    "def extract_label(json_str) :\n",
    "    json_str = json_str.replace(\"'\", \"\\\"\")\n",
    "    try:\n",
    "        json_data = json.loads(json_str)\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        if json_str[-2:] == '.}' :\n",
    "            json_str = json_str[:-2] + \".\\\"}\"\n",
    "        elif json_str[-1] == \"\\\"\" :\n",
    "            json_str = json_str + \"}\"\n",
    "        else:\n",
    "            json_str += \"\\\"}\"\n",
    "    \n",
    "    try:\n",
    "        data_dict = json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None\n",
    "\n",
    "    return data_dict[\"label\"]\n",
    "            \n",
    "def preprocessing_label(json_str) :\n",
    "    json_str = re.sub(r\"^.*### 출력\\s?:?\\s?\\n?\\s?\", \"\", str(json_str))\n",
    "    # json_str= json_str.replace(\"\\\"\", \"'\")\n",
    "    json_str = json_str.replace(\"'\", \"\\\\'\") # python interpreter에서 한번, json에서 한번\n",
    "    \n",
    "    return json_str\n",
    "\n",
    "data = remove_idx_row(data)\n",
    "\n",
    "# \"label\" column만 있다면 바꾸자.\n",
    "if \"label\" in data.columns and \"labels\" not in data.columns  :\n",
    "    data[\"labels\"] = data[\"label\"]\n",
    "\n",
    "data[\"labels\"] = data[\"labels\"].apply(preprocessing_label)\n",
    "data['labels'] = data[\"labels\"].apply(extract_label)\n",
    "data['content_corpus_company'] = data.apply(lambda row: '이 기사는 [COMPANY]'+ str(row[\"company\"]) +'[/COMPANY]에 대한 기사. [SEP]'+ extract_sentences(row['title']) + ' ' + extract_sentences(row['content_corpus']), axis=1)\n",
    "data['labels'] = data['labels'].map({'부정':0, '긍정':1})\n",
    "data = data[[\"title\", \"date\", \"content_corpus\", \"labels\", \"content_corpus_company\"]]\n",
    "print(data.shape)\n",
    "data = data[data['labels'].notna()]\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset = train_test_split(data['content_corpus'], data['labels'],\n",
    "\n",
    "# # train_dataset, test_dataset = train_test_split(data['new_column'], data['labels'],\n",
    "# #                             test_size=0.2, shuffle=True, stratify=data['labels'], # label에 비율을 맞춰서 분리\n",
    "# #                             random_state=SEED)\n",
    "\n",
    "# train_dataset, test_dataset = train_test_split(data,\n",
    "#                             test_size=0.3, shuffle=True, stratify=data['labels'], # label에 비율을 맞춰서 분리\n",
    "#                             random_state=SEED)\n",
    "\n",
    "# train_dataset, val_dataset = train_test_split(train_dataset,\n",
    "#                             test_size=0.2, shuffle=True, stratify=train_dataset['labels'], # label에 비율을 맞춰서 분리\n",
    "#                             random_state=SEED)\n",
    "\n",
    "# corpus_train, label_train = train_dataset[\"new_column\"], train_dataset[\"labels\"]\n",
    "# corpus_val, label_val = val_dataset[\"new_column\"], val_dataset[\"labels\"]\n",
    "# corpus_test, label_test = test_dataset[\"new_column\"], test_dataset[\"labels\"]\n",
    "\n",
    "# # sentence_train, sentence_val, label_train, label_val = dataset\n",
    "\n",
    "\n",
    "# max_length=40\n",
    "# stride=10\n",
    "# ## TODO 임의의 값으로 차후 수정\n",
    "# train_encoding = tokenizer(corpus_train.tolist(), ## pandas.Series -> list\n",
    "#                             return_tensors='pt',\n",
    "#                             padding=True,\n",
    "#                             truncation=True,\n",
    "#                             ##\n",
    "#                             max_length=max_length,\n",
    "#                             stride=stride,\n",
    "#                             return_overflowing_tokens=True,\n",
    "#                             return_offsets_mapping=False\n",
    "#                             )\n",
    "\n",
    "# val_encoding = tokenizer(corpus_val.tolist(),\n",
    "#                         return_tensors='pt',\n",
    "#                         padding=True,\n",
    "#                         truncation=True,\n",
    "#                         ##\n",
    "#                         max_length=max_length,\n",
    "#                         stride=stride,\n",
    "#                         return_overflowing_tokens=True,\n",
    "#                         return_offsets_mapping=False\n",
    "#                         )\n",
    "\n",
    "# train_set = SentimentalDataset(train_encoding, label_train.reset_index(drop=True))\n",
    "# val_set = SentimentalDataset(val_encoding, label_val.reset_index(drop=True))\n",
    "# test_set = SentimentalDataset(val_encoding, label_test.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_test_split(data['content_corpus_company'], data['labels'],\n",
    "                            test_size=0.2, shuffle=True, stratify=data['labels'], # label에 비율을 맞춰서 분리\n",
    "                            random_state=SEED)\n",
    "\n",
    "\n",
    "sentence_train, sentence_val, label_train, label_val = dataset\n",
    "\n",
    "\n",
    "max_length=500\n",
    "# max_length = 2000\n",
    "stride=10\n",
    "## TODO 임의의 값으로 차후 수정\n",
    "train_encoding = tokenizer(sentence_train.tolist(), ## pandas.Series -> list\n",
    "                            return_tensors='pt',\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            ##\n",
    "                            max_length=max_length,\n",
    "                            stride=stride,\n",
    "                            # return_overflowing_tokens=True,\n",
    "                            return_offsets_mapping=False\n",
    "                            )\n",
    "\n",
    "val_encoding = tokenizer(sentence_val.tolist(),\n",
    "                        return_tensors='pt',\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        ##\n",
    "                        max_length=max_length,\n",
    "                        stride=stride,\n",
    "                        # return_overflowing_tokens=True,\n",
    "                        return_offsets_mapping=False\n",
    "                        )\n",
    "\n",
    "train_set = SentimentalDataset(train_encoding, label_train.reset_index(drop=True))\n",
    "val_set = SentimentalDataset(val_encoding, label_val.reset_index(drop=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 (huggingface)\n",
    "#### hyperparameter\n",
    "- max_length\n",
    "- stride\n",
    "- num_train_epoch\n",
    "- learning_rate\n",
    "- per_device_train_batch_size\n",
    "- per_device_eval_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_steps = 200\n",
    "num_train_epochs = 3\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "learning_rate = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2271' max='2271' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2271/2271 08:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.356500</td>\n",
       "      <td>0.342270</td>\n",
       "      <td>0.912814</td>\n",
       "      <td>0.912814</td>\n",
       "      <td>0.893988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.232600</td>\n",
       "      <td>0.395361</td>\n",
       "      <td>0.918098</td>\n",
       "      <td>0.918098</td>\n",
       "      <td>0.895991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.461685</td>\n",
       "      <td>0.919419</td>\n",
       "      <td>0.919419</td>\n",
       "      <td>0.897826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2271, training_loss=0.2475055377948447, metrics={'train_runtime': 534.9292, 'train_samples_per_second': 16.982, 'train_steps_per_second': 4.245, 'total_flos': 8267250492648000.0, 'train_loss': 0.2475055377948447, 'epoch': 3.0})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run = wandb.init(project=\"final_sentimental\", entity=\"nlp-10\")\n",
    "\n",
    "# run.name = f\"model: {MODEL_NAME} / batch_size: {per_device_train_batch_size} / lr: {learning_rate}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './outputs',\n",
    "    logging_steps = logging_steps,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    per_device_eval_batch_size = per_device_eval_batch_size,\n",
    "    learning_rate = learning_rate,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print('---train start---')\n",
    "trainer.train()\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"/opt/ml/input/model-roberta_large-sota\")\n",
    "trainer.save_model(\"/opt/ml/input/model-roberta_large-sota_trainer_company-name\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sat Jul 22 12:44:18 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   41C    P0    44W / 250W |  32455MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---val evaulate start---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m---val evaulate start---\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# wandb.init()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# trainer.evaluate(eval_dataset=val_set, metric_key_prefix='val1')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mevaluate(eval_dataset\u001b[39m=\u001b[39mval_set, metric_key_prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# wandb.finish()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print('---val evaulate start---')\n",
    "# wandb.init()\n",
    "# trainer.evaluate(eval_dataset=val_set, metric_key_prefix='val1')\n",
    "# wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='303' max='303' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [303/303 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/metrics/metrics.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  acc = load_metric('accuracy').compute(predictions=preds, references=labels)['accuracy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val1_loss': 0.5156943798065186,\n",
       " 'val1_accuracy': 0.8489475856376393,\n",
       " 'val1_f1': 0.8489475856376393,\n",
       " 'val1_runtime': 12.6519,\n",
       " 'val1_samples_per_second': 191.513,\n",
       " 'val1_steps_per_second': 23.949,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=val_set, metric_key_prefix='val1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---inference start---\n",
      "######################################\n",
      "tensor([[ 3.0807, -3.1782]])\n",
      "tensor([0.9981, 0.0019])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9981, 0.0019])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('---inference start---')\n",
    "text = '이 기사는 [COMPANY]'+ '삼성전자' +'[/COMPANY]에 대한 기사. [SEP] \"\n",
    "corpus = \"삼성전자가 테슬라와 4000억 규모의 협약이 미뤄졌다.\"\n",
    "# MODEL_PATH = \"/opt/ml/input/model-roberta_large-sota_trainer\"\n",
    "MODEL_PATH = \"/opt/ml/input/model-roberta_large-sota_trainer_company-name\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "# # model = torch.load(PATH)\n",
    "model.eval()\n",
    "with torch.no_grad() :\n",
    "    temp = tokenizer(\n",
    "        my_text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        ##\n",
    "        max_length=100,\n",
    "        # stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=False\n",
    "        )\n",
    "\n",
    "    \n",
    "    temp = {\n",
    "        'input_ids':temp['input_ids'],\n",
    "        'token_type_ids':temp['token_type_ids'],\n",
    "        'attention_mask':temp['attention_mask'],\n",
    "    }\n",
    "    # print(temp)\n",
    "    \n",
    "    print(\"######################################\")\n",
    "    predicted_label = model(temp['input_ids'])\n",
    "    print(predicted_label.logits)\n",
    "    print(torch.nn.Softmax(dim=-1)(predicted_label.logits).mean(dim=0))\n",
    "    [20 80] => [50]\n",
    "    [[20, 80], [30, 70]]\n",
    "    \n",
    "\n",
    "torch.nn.Softmax(dim=-1)(predicted_label.logits).mean(dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 위에 결과에서 앞의 것이 부정 뒤에것이 긍정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정\n"
     ]
    }
   ],
   "source": [
    "result = torch.nn.Softmax(dim=-1)(predicted_label.logits).mean(dim=0)\n",
    "\n",
    "if result[0] > result[1] :\n",
    "    print(\"부정\")\n",
    "else :\n",
    "    print(\"긍정\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "981f108a204f421f158e0977940335d851edffa6dd3586828a3e1aec045160e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
