{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/final/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "from transformers import DebertaV2ForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "from dataset.datasets import SentimentalDataset\n",
    "from metrics.metrics import compute_metrics\n",
    "\n",
    "from sklearn.datasets import load_iris # ìƒ˜í”Œ ë°ì´í„° ë¡œë”©\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.utils import config_seed\n",
    "\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "config_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32002, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'klue/roberta-large'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': ['[COMPANY]','[/COMPANY]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"team-lucid/deberta-v3-base-korean\"\n",
    "\n",
    "# model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„°ì…‹ êµ¬ì„±"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) ê¸°ì‚¬ ì „ì²´ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('/opt/ml/finance_sentiment_corpus/merged_samsung_filtered.csv')\n",
    "\n",
    "# def extract_label(json_str) :\n",
    "#     data_dict = eval(json_str)  # JSON ë¬¸ìì—´ì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "#     return data_dict[\"label\"]\n",
    "\n",
    "# # \"label\" ê°’ì„ ì¶”ì¶œí•˜ì—¬ ìƒˆë¡œìš´ Series ìƒì„±\n",
    "# data['labels'] = data[\"labels\"].apply(extract_label)\n",
    "# data['labels'] = data['labels'].map({'ë¶€ì •':0, 'ê¸ì •':1})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) ê¸°ì‚¬ ì•ë’¤ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content_corpus</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì‚¼ì„±ì „ì</td>\n",
       "      <td>ë°ì´í„°ì„¼í„° ì „ë ¥ 40% ì°¨ì§€í•˜ëŠ” Dë¨â€¦ ì‚¼ì„±Â·SKí•˜ì´ë‹‰ìŠ¤, â€˜ì „ì„±ë¹„â€™ ...</td>\n",
       "      <td>2023.07.10 15:29</td>\n",
       "      <td>ì±—GPT ì‹œëŒ€ í™”ë‘ë¡œ ë– ì˜¤ë¥¸ ì „ë ¥íš¨ìœ¨ì„± ë¬¸ì œ â€ì „ë ¥ ë¨¹ëŠ” í•˜ë§ˆ, Dë¨ ì „ë ¥íš¨ìœ¨ì„± ê°œ...</td>\n",
       "      <td>{\"label\": \"ê¸ì •\", \"reason\": \"ì „ë ¥íš¨ìœ¨ì„± ê°œì„ ì„ ìœ„í•œ ì†”ë£¨ì…˜ ê°œë°œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì‚¼ì„±ì „ì</td>\n",
       "      <td>â€œì‚¼ì„±ì „ìê°€ ì‹í’ˆë„ íŒ”ì•˜ì–´?â€â€¦ì‹ ê·œ ê°€ì… ì¼ë‹¨ ì¢…ë£Œí•œ ì‚¬ì—°</td>\n",
       "      <td>2023.07.10 15:07</td>\n",
       "      <td>ì‚¼ì„± ê°€ì „ì œí’ˆ êµ¬ë§¤ê³ ê°ì— ì‚¼ì„±ë‹·ì»´ ë‚´ e-ì‹í’ˆê´€ì—ì„œ  í• ì¸í˜œíƒ ì£¼ë©° â€˜ë½ì¸â€™ ê¸°ëŒ€ ...</td>\n",
       "      <td>{\"label\": \"ë¶€ì •\", \"reason\": \"ì‚¼ì„±ì „ì ë©¤ë²„ì‹­ í”Œëœ ì‹ ê·œ ê³ ê° ëª¨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì‚¼ì„±ì „ì</td>\n",
       "      <td>SGCì†”ë£¨ì…˜ 'ë„ì–´ê¸€ë¼ìŠ¤'â€¦ì‚¼ì„±Â·LG ì„¸íƒê¸°Â·ê±´ì¡°ê¸°ì— ê³µê¸‰</td>\n",
       "      <td>2023.07.10 15:05</td>\n",
       "      <td>í•´ì™¸ ê°€ì „ ë¸Œëœë“œ ê³µëµâ€¦B2B ì‚¬ì—… í™•ì¥ SGCì†”ë£¨ì…˜ ë…¼ì‚° ê³µì¥.  .   ìƒí™œìœ ë¦¬...</td>\n",
       "      <td>{\"label\": \"ê¸ì •\", \"reason\": \"í•´ì™¸ ê°€ì „ ë¸Œëœë“œë“¤ì„ ê³µëµí•˜ë©° ì „ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì‚¼ì„±ì „ì</td>\n",
       "      <td>â€˜í˜ì´ì»¤â€™ ë‚´ì„¸ìš´ ì‚¼ì„± OLED ê²Œì´ë° ëª¨ë‹ˆí„° ê¸€ë¡œë²Œ 3ì²œëŒ€ ëŒíŒŒ</td>\n",
       "      <td>2023.07.10 14:58</td>\n",
       "      <td>ë¶ë¯¸Â·ìœ ëŸ½ ë“± ì˜ˆì•½ íŒë§¤ 3000ëŒ€ ëŒíŒŒ 10ì¼ ì˜¤í›„ 6ì‹œ ì‚¼ì„±ë‹·ì»´ â€˜í˜ì´ì»¤â€™ ì¶œì—°...</td>\n",
       "      <td>{\"label\": \"ê¸ì •\", \"reason\": \"ì˜¤ë””ì„¸ì´ OLED G9ì˜ ê¸€ë¡œë²Œ ì˜ˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì‚¼ì„±ì „ì</td>\n",
       "      <td>ë„¤ì´ì²˜ ê²Œì¬ ë“± ì„±ê³¼â€¦ì‚¼ì„±íœ´ë¨¼í…Œí¬ë…¼ë¬¸ëŒ€ìƒ, 30ë…„ ë§ì•˜ë‹¤</td>\n",
       "      <td>2023.07.10 14:48</td>\n",
       "      <td>29ë…„ê°„ 3ë§Œ6558í¸ ë…¼ë¬¸ ì ‘ìˆ˜â€¦ìˆ˜ìƒì 5312ëª… 9ì›”1ì¼ë¶€í„° ì˜¬í•´ ëŒ€ìƒ ì ‘ìˆ˜â€¦ìƒ...</td>\n",
       "      <td>{\"label\": \"ê¸ì •\", \"reason\": \"ì‚¼ì„±íœ´ë¨¼í…Œí¬ë…¼ë¬¸ëŒ€ìƒì´ ì˜¬í•´ë¡œ ì‹œí–‰ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company                                       title              date  \\\n",
       "0    ì‚¼ì„±ì „ì  ë°ì´í„°ì„¼í„° ì „ë ¥ 40% ì°¨ì§€í•˜ëŠ” Dë¨â€¦ ì‚¼ì„±Â·SKí•˜ì´ë‹‰ìŠ¤, â€˜ì „ì„±ë¹„â€™ ...  2023.07.10 15:29   \n",
       "1    ì‚¼ì„±ì „ì            â€œì‚¼ì„±ì „ìê°€ ì‹í’ˆë„ íŒ”ì•˜ì–´?â€â€¦ì‹ ê·œ ê°€ì… ì¼ë‹¨ ì¢…ë£Œí•œ ì‚¬ì—°  2023.07.10 15:07   \n",
       "2    ì‚¼ì„±ì „ì            SGCì†”ë£¨ì…˜ 'ë„ì–´ê¸€ë¼ìŠ¤'â€¦ì‚¼ì„±Â·LG ì„¸íƒê¸°Â·ê±´ì¡°ê¸°ì— ê³µê¸‰  2023.07.10 15:05   \n",
       "3    ì‚¼ì„±ì „ì        â€˜í˜ì´ì»¤â€™ ë‚´ì„¸ìš´ ì‚¼ì„± OLED ê²Œì´ë° ëª¨ë‹ˆí„° ê¸€ë¡œë²Œ 3ì²œëŒ€ ëŒíŒŒ  2023.07.10 14:58   \n",
       "4    ì‚¼ì„±ì „ì             ë„¤ì´ì²˜ ê²Œì¬ ë“± ì„±ê³¼â€¦ì‚¼ì„±íœ´ë¨¼í…Œí¬ë…¼ë¬¸ëŒ€ìƒ, 30ë…„ ë§ì•˜ë‹¤  2023.07.10 14:48   \n",
       "\n",
       "                                      content_corpus  \\\n",
       "0  ì±—GPT ì‹œëŒ€ í™”ë‘ë¡œ ë– ì˜¤ë¥¸ ì „ë ¥íš¨ìœ¨ì„± ë¬¸ì œ â€ì „ë ¥ ë¨¹ëŠ” í•˜ë§ˆ, Dë¨ ì „ë ¥íš¨ìœ¨ì„± ê°œ...   \n",
       "1  ì‚¼ì„± ê°€ì „ì œí’ˆ êµ¬ë§¤ê³ ê°ì— ì‚¼ì„±ë‹·ì»´ ë‚´ e-ì‹í’ˆê´€ì—ì„œ  í• ì¸í˜œíƒ ì£¼ë©° â€˜ë½ì¸â€™ ê¸°ëŒ€ ...   \n",
       "2  í•´ì™¸ ê°€ì „ ë¸Œëœë“œ ê³µëµâ€¦B2B ì‚¬ì—… í™•ì¥ SGCì†”ë£¨ì…˜ ë…¼ì‚° ê³µì¥.  .   ìƒí™œìœ ë¦¬...   \n",
       "3  ë¶ë¯¸Â·ìœ ëŸ½ ë“± ì˜ˆì•½ íŒë§¤ 3000ëŒ€ ëŒíŒŒ 10ì¼ ì˜¤í›„ 6ì‹œ ì‚¼ì„±ë‹·ì»´ â€˜í˜ì´ì»¤â€™ ì¶œì—°...   \n",
       "4  29ë…„ê°„ 3ë§Œ6558í¸ ë…¼ë¬¸ ì ‘ìˆ˜â€¦ìˆ˜ìƒì 5312ëª… 9ì›”1ì¼ë¶€í„° ì˜¬í•´ ëŒ€ìƒ ì ‘ìˆ˜â€¦ìƒ...   \n",
       "\n",
       "                                              labels  \n",
       "0  {\"label\": \"ê¸ì •\", \"reason\": \"ì „ë ¥íš¨ìœ¨ì„± ê°œì„ ì„ ìœ„í•œ ì†”ë£¨ì…˜ ê°œë°œ...  \n",
       "1  {\"label\": \"ë¶€ì •\", \"reason\": \"ì‚¼ì„±ì „ì ë©¤ë²„ì‹­ í”Œëœ ì‹ ê·œ ê³ ê° ëª¨...  \n",
       "2  {\"label\": \"ê¸ì •\", \"reason\": \"í•´ì™¸ ê°€ì „ ë¸Œëœë“œë“¤ì„ ê³µëµí•˜ë©° ì „ ...  \n",
       "3  {\"label\": \"ê¸ì •\", \"reason\": \"ì˜¤ë””ì„¸ì´ OLED G9ì˜ ê¸€ë¡œë²Œ ì˜ˆ...  \n",
       "4  {\"label\": \"ê¸ì •\", \"reason\": \"ì‚¼ì„±íœ´ë¨¼í…Œí¬ë…¼ë¬¸ëŒ€ìƒì´ ì˜¬í•´ë¡œ ì‹œí–‰ ...  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/opt/ml/finance_sentiment_corpus/merged/merged_all.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3778, 5)\n",
      "(3777, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content_corpus</th>\n",
       "      <th>labels</th>\n",
       "      <th>content_corpus_company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ë°ì´í„°ì„¼í„° ì „ë ¥ 40% ì°¨ì§€í•˜ëŠ” Dë¨â€¦ ì‚¼ì„±Â·SKí•˜ì´ë‹‰ìŠ¤, â€˜ì „ì„±ë¹„â€™ ...</td>\n",
       "      <td>2023.07.10 15:29</td>\n",
       "      <td>ì±—GPT ì‹œëŒ€ í™”ë‘ë¡œ ë– ì˜¤ë¥¸ ì „ë ¥íš¨ìœ¨ì„± ë¬¸ì œ â€ì „ë ¥ ë¨¹ëŠ” í•˜ë§ˆ, Dë¨ ì „ë ¥íš¨ìœ¨ì„± ê°œ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ì´ ê¸°ì‚¬ëŠ” [COMPANY]ì‚¼ì„±ì „ì[/COMPANY]ì— ëŒ€í•œ ê¸°ì‚¬. [SEP]. ë°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>â€œì‚¼ì„±ì „ìê°€ ì‹í’ˆë„ íŒ”ì•˜ì–´?â€â€¦ì‹ ê·œ ê°€ì… ì¼ë‹¨ ì¢…ë£Œí•œ ì‚¬ì—°</td>\n",
       "      <td>2023.07.10 15:07</td>\n",
       "      <td>ì‚¼ì„± ê°€ì „ì œí’ˆ êµ¬ë§¤ê³ ê°ì— ì‚¼ì„±ë‹·ì»´ ë‚´ e-ì‹í’ˆê´€ì—ì„œ  í• ì¸í˜œíƒ ì£¼ë©° â€˜ë½ì¸â€™ ê¸°ëŒ€ ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ì´ ê¸°ì‚¬ëŠ” [COMPANY]ì‚¼ì„±ì „ì[/COMPANY]ì— ëŒ€í•œ ê¸°ì‚¬. [SEP]. â€œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGCì†”ë£¨ì…˜ 'ë„ì–´ê¸€ë¼ìŠ¤'â€¦ì‚¼ì„±Â·LG ì„¸íƒê¸°Â·ê±´ì¡°ê¸°ì— ê³µê¸‰</td>\n",
       "      <td>2023.07.10 15:05</td>\n",
       "      <td>í•´ì™¸ ê°€ì „ ë¸Œëœë“œ ê³µëµâ€¦B2B ì‚¬ì—… í™•ì¥ SGCì†”ë£¨ì…˜ ë…¼ì‚° ê³µì¥.  .   ìƒí™œìœ ë¦¬...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ì´ ê¸°ì‚¬ëŠ” [COMPANY]ì‚¼ì„±ì „ì[/COMPANY]ì— ëŒ€í•œ ê¸°ì‚¬. [SEP]. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>â€˜í˜ì´ì»¤â€™ ë‚´ì„¸ìš´ ì‚¼ì„± OLED ê²Œì´ë° ëª¨ë‹ˆí„° ê¸€ë¡œë²Œ 3ì²œëŒ€ ëŒíŒŒ</td>\n",
       "      <td>2023.07.10 14:58</td>\n",
       "      <td>ë¶ë¯¸Â·ìœ ëŸ½ ë“± ì˜ˆì•½ íŒë§¤ 3000ëŒ€ ëŒíŒŒ 10ì¼ ì˜¤í›„ 6ì‹œ ì‚¼ì„±ë‹·ì»´ â€˜í˜ì´ì»¤â€™ ì¶œì—°...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ì´ ê¸°ì‚¬ëŠ” [COMPANY]ì‚¼ì„±ì „ì[/COMPANY]ì— ëŒ€í•œ ê¸°ì‚¬. [SEP]. â€˜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ë„¤ì´ì²˜ ê²Œì¬ ë“± ì„±ê³¼â€¦ì‚¼ì„±íœ´ë¨¼í…Œí¬ë…¼ë¬¸ëŒ€ìƒ, 30ë…„ ë§ì•˜ë‹¤</td>\n",
       "      <td>2023.07.10 14:48</td>\n",
       "      <td>29ë…„ê°„ 3ë§Œ6558í¸ ë…¼ë¬¸ ì ‘ìˆ˜â€¦ìˆ˜ìƒì 5312ëª… 9ì›”1ì¼ë¶€í„° ì˜¬í•´ ëŒ€ìƒ ì ‘ìˆ˜â€¦ìƒ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ì´ ê¸°ì‚¬ëŠ” [COMPANY]ì‚¼ì„±ì „ì[/COMPANY]ì— ëŒ€í•œ ê¸°ì‚¬. [SEP]. ë„¤...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        title              date  \\\n",
       "0  ë°ì´í„°ì„¼í„° ì „ë ¥ 40% ì°¨ì§€í•˜ëŠ” Dë¨â€¦ ì‚¼ì„±Â·SKí•˜ì´ë‹‰ìŠ¤, â€˜ì „ì„±ë¹„â€™ ...  2023.07.10 15:29   \n",
       "1            â€œì‚¼ì„±ì „ìê°€ ì‹í’ˆë„ íŒ”ì•˜ì–´?â€â€¦ì‹ ê·œ ê°€ì… ì¼ë‹¨ ì¢…ë£Œí•œ ì‚¬ì—°  2023.07.10 15:07   \n",
       "2            SGCì†”ë£¨ì…˜ 'ë„ì–´ê¸€ë¼ìŠ¤'â€¦ì‚¼ì„±Â·LG ì„¸íƒê¸°Â·ê±´ì¡°ê¸°ì— ê³µê¸‰  2023.07.10 15:05   \n",
       "3        â€˜í˜ì´ì»¤â€™ ë‚´ì„¸ìš´ ì‚¼ì„± OLED ê²Œì´ë° ëª¨ë‹ˆí„° ê¸€ë¡œë²Œ 3ì²œëŒ€ ëŒíŒŒ  2023.07.10 14:58   \n",
       "4             ë„¤ì´ì²˜ ê²Œì¬ ë“± ì„±ê³¼â€¦ì‚¼ì„±íœ´ë¨¼í…Œí¬ë…¼ë¬¸ëŒ€ìƒ, 30ë…„ ë§ì•˜ë‹¤  2023.07.10 14:48   \n",
       "\n",
       "                                      content_corpus  labels  \\\n",
       "0  ì±—GPT ì‹œëŒ€ í™”ë‘ë¡œ ë– ì˜¤ë¥¸ ì „ë ¥íš¨ìœ¨ì„± ë¬¸ì œ â€ì „ë ¥ ë¨¹ëŠ” í•˜ë§ˆ, Dë¨ ì „ë ¥íš¨ìœ¨ì„± ê°œ...     1.0   \n",
       "1  ì‚¼ì„± ê°€ì „ì œí’ˆ êµ¬ë§¤ê³ ê°ì— ì‚¼ì„±ë‹·ì»´ ë‚´ e-ì‹í’ˆê´€ì—ì„œ  í• ì¸í˜œíƒ ì£¼ë©° â€˜ë½ì¸â€™ ê¸°ëŒ€ ...     0.0   \n",
       "2  í•´ì™¸ ê°€ì „ ë¸Œëœë“œ ê³µëµâ€¦B2B ì‚¬ì—… í™•ì¥ SGCì†”ë£¨ì…˜ ë…¼ì‚° ê³µì¥.  .   ìƒí™œìœ ë¦¬...     1.0   \n",
       "3  ë¶ë¯¸Â·ìœ ëŸ½ ë“± ì˜ˆì•½ íŒë§¤ 3000ëŒ€ ëŒíŒŒ 10ì¼ ì˜¤í›„ 6ì‹œ ì‚¼ì„±ë‹·ì»´ â€˜í˜ì´ì»¤â€™ ì¶œì—°...     1.0   \n",
       "4  29ë…„ê°„ 3ë§Œ6558í¸ ë…¼ë¬¸ ì ‘ìˆ˜â€¦ìˆ˜ìƒì 5312ëª… 9ì›”1ì¼ë¶€í„° ì˜¬í•´ ëŒ€ìƒ ì ‘ìˆ˜â€¦ìƒ...     1.0   \n",
       "\n",
       "                              content_corpus_company  \n",
       "0  ì´ ê¸°ì‚¬ëŠ” [COMPANY]ì‚¼ì„±ì „ì[/COMPANY]ì— ëŒ€í•œ ê¸°ì‚¬. [SEP]. ë°...  \n",
       "1  ì´ ê¸°ì‚¬ëŠ” [COMPANY]ì‚¼ì„±ì „ì[/COMPANY]ì— ëŒ€í•œ ê¸°ì‚¬. [SEP]. â€œ...  \n",
       "2  ì´ ê¸°ì‚¬ëŠ” [COMPANY]ì‚¼ì„±ì „ì[/COMPANY]ì— ëŒ€í•œ ê¸°ì‚¬. [SEP]. S...  \n",
       "3  ì´ ê¸°ì‚¬ëŠ” [COMPANY]ì‚¼ì„±ì „ì[/COMPANY]ì— ëŒ€í•œ ê¸°ì‚¬. [SEP]. â€˜...  \n",
       "4  ì´ ê¸°ì‚¬ëŠ” [COMPANY]ì‚¼ì„±ì „ì[/COMPANY]ì— ëŒ€í•œ ê¸°ì‚¬. [SEP]. ë„¤...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/opt/ml/finance_sentiment_corpus/merged/merged_all.csv\")\n",
    "# data = pd.read_csv(\"/opt/ml/finance_sentiment_corpus/merged/merged_NAVER.csv\")\n",
    "\n",
    "def remove_idx_row(data) : \n",
    "    patterns = [r'idx\\s*:?\\s*.+?', r'ë¼ë²¨ë§\\s*:?\\s*.+?']\n",
    "    \n",
    "    for pattern in patterns :\n",
    "        mask = data['labels'].str.match(pattern)\n",
    "        data = data.drop(data[mask].index)\n",
    "\n",
    "    return data\n",
    "    \n",
    "# titleê³¼ content_corpusì—ì„œ ì›í•˜ëŠ” ë¬¸ì¥ ì¶”ì¶œ\n",
    "def extract_sentences(text):\n",
    "    sentences = text.split('. ')\n",
    "    if len(sentences) >= 5 :\n",
    "        return '. '.join([sentences[0], sentences[1], sentences[-2], sentences[-1]])\n",
    "    else :\n",
    "        return '. '+text\n",
    "    \n",
    "def extract_label(json_str) :\n",
    "    json_str = json_str.replace(\"'\", \"\\\"\")\n",
    "    try:\n",
    "        json_data = json.loads(json_str)\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        if json_str[-2:] == '.}' :\n",
    "            json_str = json_str[:-2] + \".\\\"}\"\n",
    "        elif json_str[-1] == \"\\\"\" :\n",
    "            json_str = json_str + \"}\"\n",
    "        else:\n",
    "            json_str += \"\\\"}\"\n",
    "    \n",
    "    try:\n",
    "        data_dict = json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None\n",
    "\n",
    "    return data_dict[\"label\"]\n",
    "            \n",
    "def preprocessing_label(json_str) :\n",
    "    json_str = re.sub(r\"^.*### ì¶œë ¥\\s?:?\\s?\\n?\\s?\", \"\", str(json_str))\n",
    "    # json_str= json_str.replace(\"\\\"\", \"'\")\n",
    "    json_str = json_str.replace(\"'\", \"\\\\'\") # python interpreterì—ì„œ í•œë²ˆ, jsonì—ì„œ í•œë²ˆ\n",
    "    \n",
    "    return json_str\n",
    "\n",
    "data = remove_idx_row(data)\n",
    "\n",
    "# \"label\" columnë§Œ ìˆë‹¤ë©´ ë°”ê¾¸ì.\n",
    "if \"label\" in data.columns and \"labels\" not in data.columns  :\n",
    "    data[\"labels\"] = data[\"label\"]\n",
    "\n",
    "data[\"labels\"] = data[\"labels\"].apply(preprocessing_label)\n",
    "data['labels'] = data[\"labels\"].apply(extract_label)\n",
    "data['content_corpus_company'] = data.apply(lambda row: 'ì´ ê¸°ì‚¬ëŠ” [COMPANY]'+ str(row[\"company\"]) +'[/COMPANY]ì— ëŒ€í•œ ê¸°ì‚¬. [SEP]'+ extract_sentences(row['title']) + ' ' + extract_sentences(row['content_corpus']), axis=1)\n",
    "data['labels'] = data['labels'].map({'ë¶€ì •':0, 'ê¸ì •':1})\n",
    "data = data[[\"title\", \"date\", \"content_corpus\", \"labels\", \"content_corpus_company\"]]\n",
    "print(data.shape)\n",
    "data = data[data['labels'].notna()]\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset = train_test_split(data['content_corpus'], data['labels'],\n",
    "\n",
    "# # train_dataset, test_dataset = train_test_split(data['new_column'], data['labels'],\n",
    "# #                             test_size=0.2, shuffle=True, stratify=data['labels'], # labelì— ë¹„ìœ¨ì„ ë§ì¶°ì„œ ë¶„ë¦¬\n",
    "# #                             random_state=SEED)\n",
    "\n",
    "# train_dataset, test_dataset = train_test_split(data,\n",
    "#                             test_size=0.3, shuffle=True, stratify=data['labels'], # labelì— ë¹„ìœ¨ì„ ë§ì¶°ì„œ ë¶„ë¦¬\n",
    "#                             random_state=SEED)\n",
    "\n",
    "# train_dataset, val_dataset = train_test_split(train_dataset,\n",
    "#                             test_size=0.2, shuffle=True, stratify=train_dataset['labels'], # labelì— ë¹„ìœ¨ì„ ë§ì¶°ì„œ ë¶„ë¦¬\n",
    "#                             random_state=SEED)\n",
    "\n",
    "# corpus_train, label_train = train_dataset[\"new_column\"], train_dataset[\"labels\"]\n",
    "# corpus_val, label_val = val_dataset[\"new_column\"], val_dataset[\"labels\"]\n",
    "# corpus_test, label_test = test_dataset[\"new_column\"], test_dataset[\"labels\"]\n",
    "\n",
    "# # sentence_train, sentence_val, label_train, label_val = dataset\n",
    "\n",
    "\n",
    "# max_length=40\n",
    "# stride=10\n",
    "# ## TODO ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì°¨í›„ ìˆ˜ì •\n",
    "# train_encoding = tokenizer(corpus_train.tolist(), ## pandas.Series -> list\n",
    "#                             return_tensors='pt',\n",
    "#                             padding=True,\n",
    "#                             truncation=True,\n",
    "#                             ##\n",
    "#                             max_length=max_length,\n",
    "#                             stride=stride,\n",
    "#                             return_overflowing_tokens=True,\n",
    "#                             return_offsets_mapping=False\n",
    "#                             )\n",
    "\n",
    "# val_encoding = tokenizer(corpus_val.tolist(),\n",
    "#                         return_tensors='pt',\n",
    "#                         padding=True,\n",
    "#                         truncation=True,\n",
    "#                         ##\n",
    "#                         max_length=max_length,\n",
    "#                         stride=stride,\n",
    "#                         return_overflowing_tokens=True,\n",
    "#                         return_offsets_mapping=False\n",
    "#                         )\n",
    "\n",
    "# train_set = SentimentalDataset(train_encoding, label_train.reset_index(drop=True))\n",
    "# val_set = SentimentalDataset(val_encoding, label_val.reset_index(drop=True))\n",
    "# test_set = SentimentalDataset(val_encoding, label_test.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_test_split(data['content_corpus_company'], data['labels'],\n",
    "                            test_size=0.2, shuffle=True, stratify=data['labels'], # labelì— ë¹„ìœ¨ì„ ë§ì¶°ì„œ ë¶„ë¦¬\n",
    "                            random_state=SEED)\n",
    "\n",
    "\n",
    "sentence_train, sentence_val, label_train, label_val = dataset\n",
    "\n",
    "\n",
    "max_length=500\n",
    "# max_length = 2000\n",
    "stride=10\n",
    "## TODO ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì°¨í›„ ìˆ˜ì •\n",
    "train_encoding = tokenizer(sentence_train.tolist(), ## pandas.Series -> list\n",
    "                            return_tensors='pt',\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            ##\n",
    "                            max_length=max_length,\n",
    "                            stride=stride,\n",
    "                            # return_overflowing_tokens=True,\n",
    "                            return_offsets_mapping=False\n",
    "                            )\n",
    "\n",
    "val_encoding = tokenizer(sentence_val.tolist(),\n",
    "                        return_tensors='pt',\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        ##\n",
    "                        max_length=max_length,\n",
    "                        stride=stride,\n",
    "                        # return_overflowing_tokens=True,\n",
    "                        return_offsets_mapping=False\n",
    "                        )\n",
    "\n",
    "train_set = SentimentalDataset(train_encoding, label_train.reset_index(drop=True))\n",
    "val_set = SentimentalDataset(val_encoding, label_val.reset_index(drop=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í•™ìŠµ (huggingface)\n",
    "#### hyperparameter\n",
    "- max_length\n",
    "- stride\n",
    "- num_train_epoch\n",
    "- learning_rate\n",
    "- per_device_train_batch_size\n",
    "- per_device_eval_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_steps = 200\n",
    "num_train_epochs = 3\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "learning_rate = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2271' max='2271' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2271/2271 08:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.356500</td>\n",
       "      <td>0.342270</td>\n",
       "      <td>0.912814</td>\n",
       "      <td>0.912814</td>\n",
       "      <td>0.893988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.232600</td>\n",
       "      <td>0.395361</td>\n",
       "      <td>0.918098</td>\n",
       "      <td>0.918098</td>\n",
       "      <td>0.895991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.461685</td>\n",
       "      <td>0.919419</td>\n",
       "      <td>0.919419</td>\n",
       "      <td>0.897826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2271, training_loss=0.2475055377948447, metrics={'train_runtime': 534.9292, 'train_samples_per_second': 16.982, 'train_steps_per_second': 4.245, 'total_flos': 8267250492648000.0, 'train_loss': 0.2475055377948447, 'epoch': 3.0})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run = wandb.init(project=\"final_sentimental\", entity=\"nlp-10\")\n",
    "\n",
    "# run.name = f\"model: {MODEL_NAME} / batch_size: {per_device_train_batch_size} / lr: {learning_rate}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './outputs',\n",
    "    logging_steps = logging_steps,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    per_device_eval_batch_size = per_device_eval_batch_size,\n",
    "    learning_rate = learning_rate,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print('---train start---')\n",
    "trainer.train()\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"/opt/ml/input/model-roberta_large-sota\")\n",
    "trainer.save_model(\"/opt/ml/input/model-roberta_large-sota_trainer_company-name\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sat Jul 22 12:44:18 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   41C    P0    44W / 250W |  32455MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---val evaulate start---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m---val evaulate start---\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# wandb.init()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# trainer.evaluate(eval_dataset=val_set, metric_key_prefix='val1')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mevaluate(eval_dataset\u001b[39m=\u001b[39mval_set, metric_key_prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# wandb.finish()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print('---val evaulate start---')\n",
    "# wandb.init()\n",
    "# trainer.evaluate(eval_dataset=val_set, metric_key_prefix='val1')\n",
    "# wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='303' max='303' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [303/303 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/metrics/metrics.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  acc = load_metric('accuracy').compute(predictions=preds, references=labels)['accuracy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val1_loss': 0.5156943798065186,\n",
       " 'val1_accuracy': 0.8489475856376393,\n",
       " 'val1_f1': 0.8489475856376393,\n",
       " 'val1_runtime': 12.6519,\n",
       " 'val1_samples_per_second': 191.513,\n",
       " 'val1_steps_per_second': 23.949,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=val_set, metric_key_prefix='val1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---inference start---\n",
      "######################################\n",
      "tensor([[ 3.0807, -3.1782]])\n",
      "tensor([0.9981, 0.0019])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9981, 0.0019])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('---inference start---')\n",
    "text = 'ì´ ê¸°ì‚¬ëŠ” [COMPANY]'+ 'ì‚¼ì„±ì „ì' +'[/COMPANY]ì— ëŒ€í•œ ê¸°ì‚¬. [SEP] \"\n",
    "corpus = \"ì‚¼ì„±ì „ìê°€ í…ŒìŠ¬ë¼ì™€ 4000ì–µ ê·œëª¨ì˜ í˜‘ì•½ì´ ë¯¸ë¤„ì¡Œë‹¤.\"\n",
    "# MODEL_PATH = \"/opt/ml/input/model-roberta_large-sota_trainer\"\n",
    "MODEL_PATH = \"/opt/ml/input/model-roberta_large-sota_trainer_company-name\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "# # model = torch.load(PATH)\n",
    "model.eval()\n",
    "with torch.no_grad() :\n",
    "    temp = tokenizer(\n",
    "        my_text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        ##\n",
    "        max_length=100,\n",
    "        # stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=False\n",
    "        )\n",
    "\n",
    "    \n",
    "    temp = {\n",
    "        'input_ids':temp['input_ids'],\n",
    "        'token_type_ids':temp['token_type_ids'],\n",
    "        'attention_mask':temp['attention_mask'],\n",
    "    }\n",
    "    # print(temp)\n",
    "    \n",
    "    print(\"######################################\")\n",
    "    predicted_label = model(temp['input_ids'])\n",
    "    print(predicted_label.logits)\n",
    "    print(torch.nn.Softmax(dim=-1)(predicted_label.logits).mean(dim=0))\n",
    "    [20 80] => [50]\n",
    "    [[20, 80], [30, 70]]\n",
    "    \n",
    "\n",
    "torch.nn.Softmax(dim=-1)(predicted_label.logits).mean(dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ìœ„ì— ê²°ê³¼ì—ì„œ ì•ì˜ ê²ƒì´ ë¶€ì • ë’¤ì—ê²ƒì´ ê¸ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¶€ì •\n"
     ]
    }
   ],
   "source": [
    "result = torch.nn.Softmax(dim=-1)(predicted_label.logits).mean(dim=0)\n",
    "\n",
    "if result[0] > result[1] :\n",
    "    print(\"ë¶€ì •\")\n",
    "else :\n",
    "    print(\"ê¸ì •\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "981f108a204f421f158e0977940335d851edffa6dd3586828a3e1aec045160e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
