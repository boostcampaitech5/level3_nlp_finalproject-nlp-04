{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "from transformers import DebertaV2ForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "from dataset.datasets import SentimentalDataset\n",
    "from metrics.metrics import compute_metrics\n",
    "\n",
    "from sklearn.datasets import load_iris # ìƒ˜í”Œ ë°ì´í„° ë¡œë”©\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.utils import config_seed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "config_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'klue/roberta-large'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"team-lucid/deberta-v3-base-korean\"\n",
    "\n",
    "# model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„°ì…‹ êµ¬ì„±"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) ê¸°ì‚¬ ì „ì²´ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('/opt/ml/finance_sentiment_corpus/merged_samsung_filtered.csv')\n",
    "\n",
    "# def extract_label(json_str) :\n",
    "#     data_dict = eval(json_str)  # JSON ë¬¸ìì—´ì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "#     return data_dict[\"label\"]\n",
    "\n",
    "# # \"label\" ê°’ì„ ì¶”ì¶œí•˜ì—¬ ìƒˆë¡œìš´ Series ìƒì„±\n",
    "# data['labels'] = data[\"labels\"].apply(extract_label)\n",
    "# data['labels'] = data['labels'].map({'ë¶€ì •':0, 'ê¸ì •':1})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) ê¸°ì‚¬ ì•ë’¤ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>content_corpus</th>\n",
       "      <th>content_len</th>\n",
       "      <th>content_corpus_len</th>\n",
       "      <th>labels</th>\n",
       "      <th>new_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ë°ì´í„°ì„¼í„° ì „ë ¥ 40% ì°¨ì§€í•˜ëŠ” Dë¨â€¦ ì‚¼ì„±Â·SKí•˜ì´ë‹‰ìŠ¤, â€˜ì „ì„±ë¹„â€™ ...</td>\n",
       "      <td>2023.07.10 15:29</td>\n",
       "      <td>ì±—GPT ì‹œëŒ€ í™”ë‘ë¡œ ë– ì˜¤ë¥¸ ì „ë ¥íš¨ìœ¨ì„± ë¬¸ì œ â€ì „ë ¥ ë¨¹ëŠ” í•˜ë§ˆ, Dë¨ ì „ë ¥íš¨ìœ¨ì„± ê°œ...</td>\n",
       "      <td>ì±—GPT ì‹œëŒ€ í™”ë‘ë¡œ ë– ì˜¤ë¥¸ ì „ë ¥íš¨ìœ¨ì„± ë¬¸ì œ â€ì „ë ¥ ë¨¹ëŠ” í•˜ë§ˆ, Dë¨ ì „ë ¥íš¨ìœ¨ì„± ê°œ...</td>\n",
       "      <td>1813</td>\n",
       "      <td>1651</td>\n",
       "      <td>1</td>\n",
       "      <td>. ë°ì´í„°ì„¼í„° ì „ë ¥ 40% ì°¨ì§€í•˜ëŠ” Dë¨â€¦ ì‚¼ì„±Â·SKí•˜ì´ë‹‰ìŠ¤, â€˜ì „ì„±ë¹„â€™ ... ì±—...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>â€œì‚¼ì„±ì „ìê°€ ì‹í’ˆë„ íŒ”ì•˜ì–´?â€â€¦ì‹ ê·œ ê°€ì… ì¼ë‹¨ ì¢…ë£Œí•œ ì‚¬ì—°</td>\n",
       "      <td>2023.07.10 15:07</td>\n",
       "      <td>ì‚¼ì„± ê°€ì „ì œí’ˆ êµ¬ë§¤ê³ ê°ì— ì‚¼ì„±ë‹·ì»´ ë‚´ e-ì‹í’ˆê´€ì—ì„œ  í• ì¸í˜œíƒ ì£¼ë©° â€˜ë½ì¸â€™ ê¸°ëŒ€ ...</td>\n",
       "      <td>ì‚¼ì„± ê°€ì „ì œí’ˆ êµ¬ë§¤ê³ ê°ì— ì‚¼ì„±ë‹·ì»´ ë‚´ e-ì‹í’ˆê´€ì—ì„œ  í• ì¸í˜œíƒ ì£¼ë©° â€˜ë½ì¸â€™ ê¸°ëŒ€ ...</td>\n",
       "      <td>1749</td>\n",
       "      <td>1698</td>\n",
       "      <td>0</td>\n",
       "      <td>. â€œì‚¼ì„±ì „ìê°€ ì‹í’ˆë„ íŒ”ì•˜ì–´?â€â€¦ì‹ ê·œ ê°€ì… ì¼ë‹¨ ì¢…ë£Œí•œ ì‚¬ì—° ì‚¼ì„± ê°€ì „ì œí’ˆ êµ¬ë§¤ê³ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>SGCì†”ë£¨ì…˜ 'ë„ì–´ê¸€ë¼ìŠ¤'â€¦ì‚¼ì„±Â·LG ì„¸íƒê¸°Â·ê±´ì¡°ê¸°ì— ê³µê¸‰</td>\n",
       "      <td>2023.07.10 15:05</td>\n",
       "      <td>í•´ì™¸ ê°€ì „ ë¸Œëœë“œ ê³µëµâ€¦B2B ì‚¬ì—… í™•ì¥[ì„œìš¸=ë‰´ì‹œìŠ¤] SGCì†”ë£¨ì…˜ ë…¼ì‚° ê³µì¥. (...</td>\n",
       "      <td>í•´ì™¸ ê°€ì „ ë¸Œëœë“œ ê³µëµâ€¦B2B ì‚¬ì—… í™•ì¥ SGCì†”ë£¨ì…˜ ë…¼ì‚° ê³µì¥.  .   ìƒí™œìœ ë¦¬...</td>\n",
       "      <td>547</td>\n",
       "      <td>476</td>\n",
       "      <td>1</td>\n",
       "      <td>. SGCì†”ë£¨ì…˜ 'ë„ì–´ê¸€ë¼ìŠ¤'â€¦ì‚¼ì„±Â·LG ì„¸íƒê¸°Â·ê±´ì¡°ê¸°ì— ê³µê¸‰ í•´ì™¸ ê°€ì „ ë¸Œëœë“œ ê³µ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>â€˜í˜ì´ì»¤â€™ ë‚´ì„¸ìš´ ì‚¼ì„± OLED ê²Œì´ë° ëª¨ë‹ˆí„° ê¸€ë¡œë²Œ 3ì²œëŒ€ ëŒíŒŒ</td>\n",
       "      <td>2023.07.10 14:58</td>\n",
       "      <td>ë¶ë¯¸Â·ìœ ëŸ½ ë“± ì˜ˆì•½ íŒë§¤ 3000ëŒ€ ëŒíŒŒ 10ì¼ ì˜¤í›„ 6ì‹œ ì‚¼ì„±ë‹·ì»´ â€˜í˜ì´ì»¤â€™ ì¶œì—°...</td>\n",
       "      <td>ë¶ë¯¸Â·ìœ ëŸ½ ë“± ì˜ˆì•½ íŒë§¤ 3000ëŒ€ ëŒíŒŒ 10ì¼ ì˜¤í›„ 6ì‹œ ì‚¼ì„±ë‹·ì»´ â€˜í˜ì´ì»¤â€™ ì¶œì—°...</td>\n",
       "      <td>1096</td>\n",
       "      <td>1029</td>\n",
       "      <td>1</td>\n",
       "      <td>. â€˜í˜ì´ì»¤â€™ ë‚´ì„¸ìš´ ì‚¼ì„± OLED ê²Œì´ë° ëª¨ë‹ˆí„° ê¸€ë¡œë²Œ 3ì²œëŒ€ ëŒíŒŒ ë¶ë¯¸Â·ìœ ëŸ½ ë“±...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>ë„¤ì´ì²˜ ê²Œì¬ ë“± ì„±ê³¼â€¦ì‚¼ì„±íœ´ë¨¼í…Œí¬ë…¼ë¬¸ëŒ€ìƒ, 30ë…„ ë§ì•˜ë‹¤</td>\n",
       "      <td>2023.07.10 14:48</td>\n",
       "      <td>29ë…„ê°„ 3ë§Œ6558í¸ ë…¼ë¬¸ ì ‘ìˆ˜â€¦ìˆ˜ìƒì 5312ëª… 9ì›”1ì¼ë¶€í„° ì˜¬í•´ ëŒ€ìƒ ì ‘ìˆ˜â€¦ìƒ...</td>\n",
       "      <td>29ë…„ê°„ 3ë§Œ6558í¸ ë…¼ë¬¸ ì ‘ìˆ˜â€¦ìˆ˜ìƒì 5312ëª… 9ì›”1ì¼ë¶€í„° ì˜¬í•´ ëŒ€ìƒ ì ‘ìˆ˜â€¦ìƒ...</td>\n",
       "      <td>1759</td>\n",
       "      <td>1659</td>\n",
       "      <td>1</td>\n",
       "      <td>. ë„¤ì´ì²˜ ê²Œì¬ ë“± ì„±ê³¼â€¦ì‚¼ì„±íœ´ë¨¼í…Œí¬ë…¼ë¬¸ëŒ€ìƒ, 30ë…„ ë§ì•˜ë‹¤ 29ë…„ê°„ 3ë§Œ6558í¸...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0             0             0           0   \n",
       "1             2             2             2           2   \n",
       "2             3             3             3           3   \n",
       "3             4             4             4           4   \n",
       "4             5             5             5           5   \n",
       "\n",
       "                                        title              date  \\\n",
       "0  ë°ì´í„°ì„¼í„° ì „ë ¥ 40% ì°¨ì§€í•˜ëŠ” Dë¨â€¦ ì‚¼ì„±Â·SKí•˜ì´ë‹‰ìŠ¤, â€˜ì „ì„±ë¹„â€™ ...  2023.07.10 15:29   \n",
       "1            â€œì‚¼ì„±ì „ìê°€ ì‹í’ˆë„ íŒ”ì•˜ì–´?â€â€¦ì‹ ê·œ ê°€ì… ì¼ë‹¨ ì¢…ë£Œí•œ ì‚¬ì—°  2023.07.10 15:07   \n",
       "2            SGCì†”ë£¨ì…˜ 'ë„ì–´ê¸€ë¼ìŠ¤'â€¦ì‚¼ì„±Â·LG ì„¸íƒê¸°Â·ê±´ì¡°ê¸°ì— ê³µê¸‰  2023.07.10 15:05   \n",
       "3        â€˜í˜ì´ì»¤â€™ ë‚´ì„¸ìš´ ì‚¼ì„± OLED ê²Œì´ë° ëª¨ë‹ˆí„° ê¸€ë¡œë²Œ 3ì²œëŒ€ ëŒíŒŒ  2023.07.10 14:58   \n",
       "4             ë„¤ì´ì²˜ ê²Œì¬ ë“± ì„±ê³¼â€¦ì‚¼ì„±íœ´ë¨¼í…Œí¬ë…¼ë¬¸ëŒ€ìƒ, 30ë…„ ë§ì•˜ë‹¤  2023.07.10 14:48   \n",
       "\n",
       "                                             content  \\\n",
       "0  ì±—GPT ì‹œëŒ€ í™”ë‘ë¡œ ë– ì˜¤ë¥¸ ì „ë ¥íš¨ìœ¨ì„± ë¬¸ì œ â€ì „ë ¥ ë¨¹ëŠ” í•˜ë§ˆ, Dë¨ ì „ë ¥íš¨ìœ¨ì„± ê°œ...   \n",
       "1  ì‚¼ì„± ê°€ì „ì œí’ˆ êµ¬ë§¤ê³ ê°ì— ì‚¼ì„±ë‹·ì»´ ë‚´ e-ì‹í’ˆê´€ì—ì„œ  í• ì¸í˜œíƒ ì£¼ë©° â€˜ë½ì¸â€™ ê¸°ëŒ€ ...   \n",
       "2  í•´ì™¸ ê°€ì „ ë¸Œëœë“œ ê³µëµâ€¦B2B ì‚¬ì—… í™•ì¥[ì„œìš¸=ë‰´ì‹œìŠ¤] SGCì†”ë£¨ì…˜ ë…¼ì‚° ê³µì¥. (...   \n",
       "3  ë¶ë¯¸Â·ìœ ëŸ½ ë“± ì˜ˆì•½ íŒë§¤ 3000ëŒ€ ëŒíŒŒ 10ì¼ ì˜¤í›„ 6ì‹œ ì‚¼ì„±ë‹·ì»´ â€˜í˜ì´ì»¤â€™ ì¶œì—°...   \n",
       "4  29ë…„ê°„ 3ë§Œ6558í¸ ë…¼ë¬¸ ì ‘ìˆ˜â€¦ìˆ˜ìƒì 5312ëª… 9ì›”1ì¼ë¶€í„° ì˜¬í•´ ëŒ€ìƒ ì ‘ìˆ˜â€¦ìƒ...   \n",
       "\n",
       "                                      content_corpus  content_len  \\\n",
       "0  ì±—GPT ì‹œëŒ€ í™”ë‘ë¡œ ë– ì˜¤ë¥¸ ì „ë ¥íš¨ìœ¨ì„± ë¬¸ì œ â€ì „ë ¥ ë¨¹ëŠ” í•˜ë§ˆ, Dë¨ ì „ë ¥íš¨ìœ¨ì„± ê°œ...         1813   \n",
       "1  ì‚¼ì„± ê°€ì „ì œí’ˆ êµ¬ë§¤ê³ ê°ì— ì‚¼ì„±ë‹·ì»´ ë‚´ e-ì‹í’ˆê´€ì—ì„œ  í• ì¸í˜œíƒ ì£¼ë©° â€˜ë½ì¸â€™ ê¸°ëŒ€ ...         1749   \n",
       "2  í•´ì™¸ ê°€ì „ ë¸Œëœë“œ ê³µëµâ€¦B2B ì‚¬ì—… í™•ì¥ SGCì†”ë£¨ì…˜ ë…¼ì‚° ê³µì¥.  .   ìƒí™œìœ ë¦¬...          547   \n",
       "3  ë¶ë¯¸Â·ìœ ëŸ½ ë“± ì˜ˆì•½ íŒë§¤ 3000ëŒ€ ëŒíŒŒ 10ì¼ ì˜¤í›„ 6ì‹œ ì‚¼ì„±ë‹·ì»´ â€˜í˜ì´ì»¤â€™ ì¶œì—°...         1096   \n",
       "4  29ë…„ê°„ 3ë§Œ6558í¸ ë…¼ë¬¸ ì ‘ìˆ˜â€¦ìˆ˜ìƒì 5312ëª… 9ì›”1ì¼ë¶€í„° ì˜¬í•´ ëŒ€ìƒ ì ‘ìˆ˜â€¦ìƒ...         1759   \n",
       "\n",
       "   content_corpus_len  labels  \\\n",
       "0                1651       1   \n",
       "1                1698       0   \n",
       "2                 476       1   \n",
       "3                1029       1   \n",
       "4                1659       1   \n",
       "\n",
       "                                          new_column  \n",
       "0  . ë°ì´í„°ì„¼í„° ì „ë ¥ 40% ì°¨ì§€í•˜ëŠ” Dë¨â€¦ ì‚¼ì„±Â·SKí•˜ì´ë‹‰ìŠ¤, â€˜ì „ì„±ë¹„â€™ ... ì±—...  \n",
       "1  . â€œì‚¼ì„±ì „ìê°€ ì‹í’ˆë„ íŒ”ì•˜ì–´?â€â€¦ì‹ ê·œ ê°€ì… ì¼ë‹¨ ì¢…ë£Œí•œ ì‚¬ì—° ì‚¼ì„± ê°€ì „ì œí’ˆ êµ¬ë§¤ê³ ...  \n",
       "2  . SGCì†”ë£¨ì…˜ 'ë„ì–´ê¸€ë¼ìŠ¤'â€¦ì‚¼ì„±Â·LG ì„¸íƒê¸°Â·ê±´ì¡°ê¸°ì— ê³µê¸‰ í•´ì™¸ ê°€ì „ ë¸Œëœë“œ ê³µ...  \n",
       "3  . â€˜í˜ì´ì»¤â€™ ë‚´ì„¸ìš´ ì‚¼ì„± OLED ê²Œì´ë° ëª¨ë‹ˆí„° ê¸€ë¡œë²Œ 3ì²œëŒ€ ëŒíŒŒ ë¶ë¯¸Â·ìœ ëŸ½ ë“±...  \n",
       "4  . ë„¤ì´ì²˜ ê²Œì¬ ë“± ì„±ê³¼â€¦ì‚¼ì„±íœ´ë¨¼í…Œí¬ë…¼ë¬¸ëŒ€ìƒ, 30ë…„ ë§ì•˜ë‹¤ 29ë…„ê°„ 3ë§Œ6558í¸...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/opt/ml/finance_sentiment_corpus/merged_samsung_filtered.csv')\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# titleê³¼ content_corpusì—ì„œ ì›í•˜ëŠ” ë¬¸ì¥ ì¶”ì¶œ\n",
    "def extract_sentences(text):\n",
    "    sentences = text.split('. ')\n",
    "    if len(sentences) >= 5 :\n",
    "        return '. '.join([sentences[0], sentences[1], sentences[-2], sentences[-1]])\n",
    "    else :\n",
    "        return '. '+text\n",
    "    \n",
    "def extract_label(json_str) :\n",
    "    data_dict = eval(json_str)  # JSON ë¬¸ìì—´ì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "    return data_dict[\"label\"]\n",
    "\n",
    "# \"label\" ê°’ì„ ì¶”ì¶œí•˜ì—¬ ìƒˆë¡œìš´ Series ìƒì„±\n",
    "data['labels'] = data[\"labels\"].apply(extract_label)\n",
    "data['new_column'] = data.apply(lambda row: extract_sentences(row['title']) + ' ' + extract_sentences(row['content_corpus']), axis=1)\n",
    "data['labels'] = data['labels'].map({'ë¶€ì •':0, 'ê¸ì •':1})\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset = train_test_split(data['content_corpus'], data['labels'],\n",
    "\n",
    "# # train_dataset, test_dataset = train_test_split(data['new_column'], data['labels'],\n",
    "# #                             test_size=0.2, shuffle=True, stratify=data['labels'], # labelì— ë¹„ìœ¨ì„ ë§ì¶°ì„œ ë¶„ë¦¬\n",
    "# #                             random_state=SEED)\n",
    "\n",
    "# train_dataset, test_dataset = train_test_split(data,\n",
    "#                             test_size=0.3, shuffle=True, stratify=data['labels'], # labelì— ë¹„ìœ¨ì„ ë§ì¶°ì„œ ë¶„ë¦¬\n",
    "#                             random_state=SEED)\n",
    "\n",
    "# train_dataset, val_dataset = train_test_split(train_dataset,\n",
    "#                             test_size=0.2, shuffle=True, stratify=train_dataset['labels'], # labelì— ë¹„ìœ¨ì„ ë§ì¶°ì„œ ë¶„ë¦¬\n",
    "#                             random_state=SEED)\n",
    "\n",
    "# corpus_train, label_train = train_dataset[\"new_column\"], train_dataset[\"labels\"]\n",
    "# corpus_val, label_val = val_dataset[\"new_column\"], val_dataset[\"labels\"]\n",
    "# corpus_test, label_test = test_dataset[\"new_column\"], test_dataset[\"labels\"]\n",
    "\n",
    "# # sentence_train, sentence_val, label_train, label_val = dataset\n",
    "\n",
    "\n",
    "# max_length=40\n",
    "# stride=10\n",
    "# ## TODO ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì°¨í›„ ìˆ˜ì •\n",
    "# train_encoding = tokenizer(corpus_train.tolist(), ## pandas.Series -> list\n",
    "#                             return_tensors='pt',\n",
    "#                             padding=True,\n",
    "#                             truncation=True,\n",
    "#                             ##\n",
    "#                             max_length=max_length,\n",
    "#                             stride=stride,\n",
    "#                             return_overflowing_tokens=True,\n",
    "#                             return_offsets_mapping=False\n",
    "#                             )\n",
    "\n",
    "# val_encoding = tokenizer(corpus_val.tolist(),\n",
    "#                         return_tensors='pt',\n",
    "#                         padding=True,\n",
    "#                         truncation=True,\n",
    "#                         ##\n",
    "#                         max_length=max_length,\n",
    "#                         stride=stride,\n",
    "#                         return_overflowing_tokens=True,\n",
    "#                         return_offsets_mapping=False\n",
    "#                         )\n",
    "\n",
    "# train_set = SentimentalDataset(train_encoding, label_train.reset_index(drop=True))\n",
    "# val_set = SentimentalDataset(val_encoding, label_val.reset_index(drop=True))\n",
    "# test_set = SentimentalDataset(val_encoding, label_test.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = train_test_split(data['content_corpus'], data['labels'],\n",
    "\n",
    "dataset = train_test_split(data['new_column'], data['labels'],\n",
    "                            test_size=0.2, shuffle=True, stratify=data['labels'], # labelì— ë¹„ìœ¨ì„ ë§ì¶°ì„œ ë¶„ë¦¬\n",
    "                            random_state=SEED)\n",
    "\n",
    "\n",
    "sentence_train, sentence_val, label_train, label_val = dataset\n",
    "\n",
    "\n",
    "max_length=500\n",
    "# max_length = 2000\n",
    "stride=10\n",
    "## TODO ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì°¨í›„ ìˆ˜ì •\n",
    "train_encoding = tokenizer(sentence_train.tolist(), ## pandas.Series -> list\n",
    "                            return_tensors='pt',\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            ##\n",
    "                            max_length=max_length,\n",
    "                            stride=stride,\n",
    "                            return_overflowing_tokens=True,\n",
    "                            return_offsets_mapping=False\n",
    "                            )\n",
    "\n",
    "val_encoding = tokenizer(sentence_val.tolist(),\n",
    "                        return_tensors='pt',\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        ##\n",
    "                        max_length=max_length,\n",
    "                        stride=stride,\n",
    "                        return_overflowing_tokens=True,\n",
    "                        return_offsets_mapping=False\n",
    "                        )\n",
    "\n",
    "train_set = SentimentalDataset(train_encoding, label_train.reset_index(drop=True))\n",
    "val_set = SentimentalDataset(val_encoding, label_val.reset_index(drop=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í•™ìŠµ (huggingface)\n",
    "#### hyperparameter\n",
    "- max_length\n",
    "- stride\n",
    "- num_train_epoch\n",
    "- learning_rate\n",
    "- per_device_train_batch_size\n",
    "- per_device_eval_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_steps = 200\n",
    "num_train_epochs = 3\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "learning_rate = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1095' max='1095' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1095/1095 05:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.476500</td>\n",
       "      <td>0.297409</td>\n",
       "      <td>0.920548</td>\n",
       "      <td>0.920548</td>\n",
       "      <td>0.899829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.287600</td>\n",
       "      <td>0.370972</td>\n",
       "      <td>0.915068</td>\n",
       "      <td>0.915068</td>\n",
       "      <td>0.890825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.175200</td>\n",
       "      <td>0.375522</td>\n",
       "      <td>0.934247</td>\n",
       "      <td>0.934247</td>\n",
       "      <td>0.918363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1095, training_loss=0.3064597926727713, metrics={'train_runtime': 309.5533, 'train_samples_per_second': 14.149, 'train_steps_per_second': 3.537, 'total_flos': 3986190792360000.0, 'train_loss': 0.3064597926727713, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run = wandb.init(project=\"final_sentimental\", entity=\"nlp-10\")\n",
    "\n",
    "# run.name = f\"model: {MODEL_NAME} / batch_size: {per_device_train_batch_size} / lr: {learning_rate}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './outputs',\n",
    "    logging_steps = logging_steps,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    per_device_eval_batch_size = per_device_eval_batch_size,\n",
    "    learning_rate = learning_rate,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print('---train start---')\n",
    "trainer.train()\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"/opt/ml/input/model-roberta_large-sota\")\n",
    "trainer.save_model(\"/opt/ml/input/model-roberta_large-sota_trainer\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sat Jul 22 12:44:18 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   41C    P0    44W / 250W |  32455MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---val evaulate start---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m---val evaulate start---\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# wandb.init()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# trainer.evaluate(eval_dataset=val_set, metric_key_prefix='val1')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mevaluate(eval_dataset\u001b[39m=\u001b[39mval_set, metric_key_prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# wandb.finish()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print('---val evaulate start---')\n",
    "# wandb.init()\n",
    "# trainer.evaluate(eval_dataset=val_set, metric_key_prefix='val1')\n",
    "# wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í•™ìŠµ(wandb_sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {'name': f\"{MODEL_NAME}_1\",  # name : sweep_name\n",
    "                    'method': 'grid',  # 'grid', 'uniform', 'bayesian'\n",
    "                    'parameters': {\n",
    "                        'lr': {  # parameter  ì‘ì„±ë°©ì‹ ì—¬ëŸ¬ê°œ ìˆìœ¼ë‹ˆê¹Œ, ë…¸ì…˜ ë¬¸ì„œ ì°¸ê³ \n",
    "                            'values': [4e-6, 5e-6, 6e-6]\n",
    "                        },\n",
    "                        'warmup_step': {\n",
    "                            \"values\": [200]\n",
    "                        },\n",
    "                        'logging_steps': {\n",
    "                            \"values\": [200]\n",
    "                        },\n",
    "                        \"batch_size\": {\n",
    "                            \"values\": [8]\n",
    "                        },\n",
    "                        \"max_epoch\": {\n",
    "                            \"values\": [2]\n",
    "                        },\n",
    "                    },\n",
    "                    # goal : maximize, minimize\n",
    "                    'metric': {'name': 'val1_accuracy', 'goal': 'maximize'}\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mode\n",
      "Create sweep with ID: tdlke131\n",
      "Sweep URL: https://wandb.ai/nlp-10/final_sentimental/sweeps/tdlke131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# print(\"Training with sweep mode\")\n",
    "\n",
    "# sweep_id = wandb.sweep(json.load(open(\"/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/sweep.json\", \"r\"), object_hook=_decode), project=\"final_sentimental\", entity=\"nlp-10\")\n",
    "# wandb.agent(sweep_id, trainWithSweep, count=int(5))\n",
    "# wandb.finish()\n",
    "from train import sweep_train\n",
    "\n",
    "print(\"Train mode\")\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"final_sentimental\", entity=\"nlp-10\")\n",
    "wandb.agent(sweep_id, sweep_train, count=3)\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='303' max='303' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [303/303 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/metrics/metrics.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  acc = load_metric('accuracy').compute(predictions=preds, references=labels)['accuracy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val1_loss': 0.5156943798065186,\n",
       " 'val1_accuracy': 0.8489475856376393,\n",
       " 'val1_f1': 0.8489475856376393,\n",
       " 'val1_runtime': 12.6519,\n",
       " 'val1_samples_per_second': 191.513,\n",
       " 'val1_steps_per_second': 23.949,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---inference start---\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '/opt/ml/input/model-roberta_large-sota_trainer'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/opt/ml/input/model-roberta_large-sota_trainer' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# model = model.to('cpu')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# MODEL_PATH = \"/opt/ml/input/model-roberta_large-sota_trainer/pytorch_model.bin\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m MODEL_PATH \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/opt/ml/input/model-roberta_large-sota_trainer\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(MODEL_PATH)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# tokenizer = RobertaTokenizer(MODEL_PATH)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# model = RobertaForSequenceClassification(MODEL_PATH)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(MODEL_PATH)\n",
      "File \u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:693\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    690\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    691\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_candidate\u001b[39m}\u001b[39;00m\u001b[39m does not exist or is not currently imported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 693\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    695\u001b[0m \u001b[39m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[39m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1796\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1790\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1791\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1792\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1795\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1796\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1797\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1798\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1799\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1800\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1801\u001b[0m     )\n\u001b[1;32m   1803\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1804\u001b[0m     \u001b[39mif\u001b[39;00m file_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/opt/ml/input/model-roberta_large-sota_trainer'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/opt/ml/input/model-roberta_large-sota_trainer' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "print('---inference start---')\n",
    "my_text = 'ì‚¼ì„±ì „ì ì§„ì§œ ë¬´ì¡°ê±´ ì˜¤ë¥¼ ë“¯'*20\n",
    "\n",
    "MODEL_PATH = \"/opt/ml/input/model-roberta_large-sota_trainer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "# # model = torch.load(PATH)\n",
    "model.eval()\n",
    "with torch.no_grad() :\n",
    "    temp = tokenizer(\n",
    "        my_text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        ##\n",
    "        max_length=100,\n",
    "        # stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=False\n",
    "        )\n",
    "\n",
    "    \n",
    "    temp = {\n",
    "        'input_ids':temp['input_ids'],\n",
    "        'token_type_ids':temp['token_type_ids'],\n",
    "        'attention_mask':temp['attention_mask'],\n",
    "    }\n",
    "    # print(temp)\n",
    "    \n",
    "    print(\"######################################\")\n",
    "    predicted_label = model(temp['input_ids'])\n",
    "    print(predicted_label.logits)\n",
    "    print(torch.nn.Softmax(dim=-1)(predicted_label.logits).mean(dim=0))\n",
    "\n",
    "torch.nn.Softmax(dim=-1)(predicted_label.logits).mean(dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ìœ„ì— ê²°ê³¼ì—ì„œ ì•ì˜ ê²ƒì´ ë¶€ì • ë’¤ì—ê²ƒì´ ê¸ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¶€ì •\n"
     ]
    }
   ],
   "source": [
    "result = torch.nn.Softmax(dim=-1)(predicted_label.logits).mean(dim=0)\n",
    "\n",
    "if result[0] > result[1] :\n",
    "    print(\"ë¶€ì •\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "981f108a204f421f158e0977940335d851edffa6dd3586828a3e1aec045160e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
