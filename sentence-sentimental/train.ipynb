{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "from transformers import DebertaV2ForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "from dataset.datasets import SentimentalDataset\n",
    "from metrics.metrics import compute_metrics\n",
    "\n",
    "from sklearn.datasets import load_iris # 샘플 데이터 로딩\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.utils import config_seed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "config_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 및 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'klue/roberta-large'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"team-lucid/deberta-v3-base-korean\"\n",
    "\n",
    "# model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 구성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 기사 전체 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('/opt/ml/finance_sentiment_corpus/merged_samsung_filtered.csv')\n",
    "\n",
    "# def extract_label(json_str) :\n",
    "#     data_dict = eval(json_str)  # JSON 문자열을 파이썬 딕셔너리로 변환\n",
    "#     return data_dict[\"label\"]\n",
    "\n",
    "# # \"label\" 값을 추출하여 새로운 Series 생성\n",
    "# data['labels'] = data[\"labels\"].apply(extract_label)\n",
    "# data['labels'] = data['labels'].map({'부정':0, '긍정':1})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 기사 앞뒤 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>content_corpus</th>\n",
       "      <th>content_len</th>\n",
       "      <th>content_corpus_len</th>\n",
       "      <th>labels</th>\n",
       "      <th>new_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>데이터센터 전력 40% 차지하는 D램… 삼성·SK하이닉스, ‘전성비’ ...</td>\n",
       "      <td>2023.07.10 15:29</td>\n",
       "      <td>챗GPT 시대 화두로 떠오른 전력효율성 문제 ”전력 먹는 하마, D램 전력효율성 개...</td>\n",
       "      <td>챗GPT 시대 화두로 떠오른 전력효율성 문제 ”전력 먹는 하마, D램 전력효율성 개...</td>\n",
       "      <td>1813</td>\n",
       "      <td>1651</td>\n",
       "      <td>1</td>\n",
       "      <td>. 데이터센터 전력 40% 차지하는 D램… 삼성·SK하이닉스, ‘전성비’ ... 챗...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>“삼성전자가 식품도 팔았어?”…신규 가입 일단 종료한 사연</td>\n",
       "      <td>2023.07.10 15:07</td>\n",
       "      <td>삼성 가전제품 구매고객에 삼성닷컴 내 e-식품관에서  할인혜택 주며 ‘락인’ 기대 ...</td>\n",
       "      <td>삼성 가전제품 구매고객에 삼성닷컴 내 e-식품관에서  할인혜택 주며 ‘락인’ 기대 ...</td>\n",
       "      <td>1749</td>\n",
       "      <td>1698</td>\n",
       "      <td>0</td>\n",
       "      <td>. “삼성전자가 식품도 팔았어?”…신규 가입 일단 종료한 사연 삼성 가전제품 구매고...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>SGC솔루션 '도어글라스'…삼성·LG 세탁기·건조기에 공급</td>\n",
       "      <td>2023.07.10 15:05</td>\n",
       "      <td>해외 가전 브랜드 공략…B2B 사업 확장[서울=뉴시스] SGC솔루션 논산 공장. (...</td>\n",
       "      <td>해외 가전 브랜드 공략…B2B 사업 확장 SGC솔루션 논산 공장.  .   생활유리...</td>\n",
       "      <td>547</td>\n",
       "      <td>476</td>\n",
       "      <td>1</td>\n",
       "      <td>. SGC솔루션 '도어글라스'…삼성·LG 세탁기·건조기에 공급 해외 가전 브랜드 공...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>‘페이커’ 내세운 삼성 OLED 게이밍 모니터 글로벌 3천대 돌파</td>\n",
       "      <td>2023.07.10 14:58</td>\n",
       "      <td>북미·유럽 등 예약 판매 3000대 돌파 10일 오후 6시 삼성닷컴 ‘페이커’ 출연...</td>\n",
       "      <td>북미·유럽 등 예약 판매 3000대 돌파 10일 오후 6시 삼성닷컴 ‘페이커’ 출연...</td>\n",
       "      <td>1096</td>\n",
       "      <td>1029</td>\n",
       "      <td>1</td>\n",
       "      <td>. ‘페이커’ 내세운 삼성 OLED 게이밍 모니터 글로벌 3천대 돌파 북미·유럽 등...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>네이처 게재 등 성과…삼성휴먼테크논문대상, 30년 맞았다</td>\n",
       "      <td>2023.07.10 14:48</td>\n",
       "      <td>29년간 3만6558편 논문 접수…수상자 5312명 9월1일부터 올해 대상 접수…상...</td>\n",
       "      <td>29년간 3만6558편 논문 접수…수상자 5312명 9월1일부터 올해 대상 접수…상...</td>\n",
       "      <td>1759</td>\n",
       "      <td>1659</td>\n",
       "      <td>1</td>\n",
       "      <td>. 네이처 게재 등 성과…삼성휴먼테크논문대상, 30년 맞았다 29년간 3만6558편...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0             0             0           0   \n",
       "1             2             2             2           2   \n",
       "2             3             3             3           3   \n",
       "3             4             4             4           4   \n",
       "4             5             5             5           5   \n",
       "\n",
       "                                        title              date  \\\n",
       "0  데이터센터 전력 40% 차지하는 D램… 삼성·SK하이닉스, ‘전성비’ ...  2023.07.10 15:29   \n",
       "1            “삼성전자가 식품도 팔았어?”…신규 가입 일단 종료한 사연  2023.07.10 15:07   \n",
       "2            SGC솔루션 '도어글라스'…삼성·LG 세탁기·건조기에 공급  2023.07.10 15:05   \n",
       "3        ‘페이커’ 내세운 삼성 OLED 게이밍 모니터 글로벌 3천대 돌파  2023.07.10 14:58   \n",
       "4             네이처 게재 등 성과…삼성휴먼테크논문대상, 30년 맞았다  2023.07.10 14:48   \n",
       "\n",
       "                                             content  \\\n",
       "0  챗GPT 시대 화두로 떠오른 전력효율성 문제 ”전력 먹는 하마, D램 전력효율성 개...   \n",
       "1  삼성 가전제품 구매고객에 삼성닷컴 내 e-식품관에서  할인혜택 주며 ‘락인’ 기대 ...   \n",
       "2  해외 가전 브랜드 공략…B2B 사업 확장[서울=뉴시스] SGC솔루션 논산 공장. (...   \n",
       "3  북미·유럽 등 예약 판매 3000대 돌파 10일 오후 6시 삼성닷컴 ‘페이커’ 출연...   \n",
       "4  29년간 3만6558편 논문 접수…수상자 5312명 9월1일부터 올해 대상 접수…상...   \n",
       "\n",
       "                                      content_corpus  content_len  \\\n",
       "0  챗GPT 시대 화두로 떠오른 전력효율성 문제 ”전력 먹는 하마, D램 전력효율성 개...         1813   \n",
       "1  삼성 가전제품 구매고객에 삼성닷컴 내 e-식품관에서  할인혜택 주며 ‘락인’ 기대 ...         1749   \n",
       "2  해외 가전 브랜드 공략…B2B 사업 확장 SGC솔루션 논산 공장.  .   생활유리...          547   \n",
       "3  북미·유럽 등 예약 판매 3000대 돌파 10일 오후 6시 삼성닷컴 ‘페이커’ 출연...         1096   \n",
       "4  29년간 3만6558편 논문 접수…수상자 5312명 9월1일부터 올해 대상 접수…상...         1759   \n",
       "\n",
       "   content_corpus_len  labels  \\\n",
       "0                1651       1   \n",
       "1                1698       0   \n",
       "2                 476       1   \n",
       "3                1029       1   \n",
       "4                1659       1   \n",
       "\n",
       "                                          new_column  \n",
       "0  . 데이터센터 전력 40% 차지하는 D램… 삼성·SK하이닉스, ‘전성비’ ... 챗...  \n",
       "1  . “삼성전자가 식품도 팔았어?”…신규 가입 일단 종료한 사연 삼성 가전제품 구매고...  \n",
       "2  . SGC솔루션 '도어글라스'…삼성·LG 세탁기·건조기에 공급 해외 가전 브랜드 공...  \n",
       "3  . ‘페이커’ 내세운 삼성 OLED 게이밍 모니터 글로벌 3천대 돌파 북미·유럽 등...  \n",
       "4  . 네이처 게재 등 성과…삼성휴먼테크논문대상, 30년 맞았다 29년간 3만6558편...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/opt/ml/finance_sentiment_corpus/merged_samsung_filtered.csv')\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# title과 content_corpus에서 원하는 문장 추출\n",
    "def extract_sentences(text):\n",
    "    sentences = text.split('. ')\n",
    "    if len(sentences) >= 5 :\n",
    "        return '. '.join([sentences[0], sentences[1], sentences[-2], sentences[-1]])\n",
    "    else :\n",
    "        return '. '+text\n",
    "    \n",
    "def extract_label(json_str) :\n",
    "    data_dict = eval(json_str)  # JSON 문자열을 파이썬 딕셔너리로 변환\n",
    "    return data_dict[\"label\"]\n",
    "\n",
    "# \"label\" 값을 추출하여 새로운 Series 생성\n",
    "data['labels'] = data[\"labels\"].apply(extract_label)\n",
    "data['new_column'] = data.apply(lambda row: extract_sentences(row['title']) + ' ' + extract_sentences(row['content_corpus']), axis=1)\n",
    "data['labels'] = data['labels'].map({'부정':0, '긍정':1})\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset = train_test_split(data['content_corpus'], data['labels'],\n",
    "\n",
    "# # train_dataset, test_dataset = train_test_split(data['new_column'], data['labels'],\n",
    "# #                             test_size=0.2, shuffle=True, stratify=data['labels'], # label에 비율을 맞춰서 분리\n",
    "# #                             random_state=SEED)\n",
    "\n",
    "# train_dataset, test_dataset = train_test_split(data,\n",
    "#                             test_size=0.3, shuffle=True, stratify=data['labels'], # label에 비율을 맞춰서 분리\n",
    "#                             random_state=SEED)\n",
    "\n",
    "# train_dataset, val_dataset = train_test_split(train_dataset,\n",
    "#                             test_size=0.2, shuffle=True, stratify=train_dataset['labels'], # label에 비율을 맞춰서 분리\n",
    "#                             random_state=SEED)\n",
    "\n",
    "# corpus_train, label_train = train_dataset[\"new_column\"], train_dataset[\"labels\"]\n",
    "# corpus_val, label_val = val_dataset[\"new_column\"], val_dataset[\"labels\"]\n",
    "# corpus_test, label_test = test_dataset[\"new_column\"], test_dataset[\"labels\"]\n",
    "\n",
    "# # sentence_train, sentence_val, label_train, label_val = dataset\n",
    "\n",
    "\n",
    "# max_length=40\n",
    "# stride=10\n",
    "# ## TODO 임의의 값으로 차후 수정\n",
    "# train_encoding = tokenizer(corpus_train.tolist(), ## pandas.Series -> list\n",
    "#                             return_tensors='pt',\n",
    "#                             padding=True,\n",
    "#                             truncation=True,\n",
    "#                             ##\n",
    "#                             max_length=max_length,\n",
    "#                             stride=stride,\n",
    "#                             return_overflowing_tokens=True,\n",
    "#                             return_offsets_mapping=False\n",
    "#                             )\n",
    "\n",
    "# val_encoding = tokenizer(corpus_val.tolist(),\n",
    "#                         return_tensors='pt',\n",
    "#                         padding=True,\n",
    "#                         truncation=True,\n",
    "#                         ##\n",
    "#                         max_length=max_length,\n",
    "#                         stride=stride,\n",
    "#                         return_overflowing_tokens=True,\n",
    "#                         return_offsets_mapping=False\n",
    "#                         )\n",
    "\n",
    "# train_set = SentimentalDataset(train_encoding, label_train.reset_index(drop=True))\n",
    "# val_set = SentimentalDataset(val_encoding, label_val.reset_index(drop=True))\n",
    "# test_set = SentimentalDataset(val_encoding, label_test.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = train_test_split(data['content_corpus'], data['labels'],\n",
    "\n",
    "dataset = train_test_split(data['new_column'], data['labels'],\n",
    "                            test_size=0.2, shuffle=True, stratify=data['labels'], # label에 비율을 맞춰서 분리\n",
    "                            random_state=SEED)\n",
    "\n",
    "\n",
    "sentence_train, sentence_val, label_train, label_val = dataset\n",
    "\n",
    "\n",
    "max_length=500\n",
    "# max_length = 2000\n",
    "stride=10\n",
    "## TODO 임의의 값으로 차후 수정\n",
    "train_encoding = tokenizer(sentence_train.tolist(), ## pandas.Series -> list\n",
    "                            return_tensors='pt',\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            ##\n",
    "                            max_length=max_length,\n",
    "                            stride=stride,\n",
    "                            return_overflowing_tokens=True,\n",
    "                            return_offsets_mapping=False\n",
    "                            )\n",
    "\n",
    "val_encoding = tokenizer(sentence_val.tolist(),\n",
    "                        return_tensors='pt',\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        ##\n",
    "                        max_length=max_length,\n",
    "                        stride=stride,\n",
    "                        return_overflowing_tokens=True,\n",
    "                        return_offsets_mapping=False\n",
    "                        )\n",
    "\n",
    "train_set = SentimentalDataset(train_encoding, label_train.reset_index(drop=True))\n",
    "val_set = SentimentalDataset(val_encoding, label_val.reset_index(drop=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 (huggingface)\n",
    "#### hyperparameter\n",
    "- max_length\n",
    "- stride\n",
    "- num_train_epoch\n",
    "- learning_rate\n",
    "- per_device_train_batch_size\n",
    "- per_device_eval_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_steps = 200\n",
    "num_train_epochs = 3\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "learning_rate = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1095' max='1095' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1095/1095 05:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.476500</td>\n",
       "      <td>0.297409</td>\n",
       "      <td>0.920548</td>\n",
       "      <td>0.920548</td>\n",
       "      <td>0.899829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.287600</td>\n",
       "      <td>0.370972</td>\n",
       "      <td>0.915068</td>\n",
       "      <td>0.915068</td>\n",
       "      <td>0.890825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.175200</td>\n",
       "      <td>0.375522</td>\n",
       "      <td>0.934247</td>\n",
       "      <td>0.934247</td>\n",
       "      <td>0.918363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1095, training_loss=0.3064597926727713, metrics={'train_runtime': 309.5533, 'train_samples_per_second': 14.149, 'train_steps_per_second': 3.537, 'total_flos': 3986190792360000.0, 'train_loss': 0.3064597926727713, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run = wandb.init(project=\"final_sentimental\", entity=\"nlp-10\")\n",
    "\n",
    "# run.name = f\"model: {MODEL_NAME} / batch_size: {per_device_train_batch_size} / lr: {learning_rate}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './outputs',\n",
    "    logging_steps = logging_steps,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    per_device_eval_batch_size = per_device_eval_batch_size,\n",
    "    learning_rate = learning_rate,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print('---train start---')\n",
    "trainer.train()\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"/opt/ml/input/model-roberta_large-sota\")\n",
    "trainer.save_model(\"/opt/ml/input/model-roberta_large-sota_trainer\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sat Jul 22 12:44:18 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   41C    P0    44W / 250W |  32455MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---val evaulate start---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m---val evaulate start---\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# wandb.init()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# trainer.evaluate(eval_dataset=val_set, metric_key_prefix='val1')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mevaluate(eval_dataset\u001b[39m=\u001b[39mval_set, metric_key_prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# wandb.finish()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print('---val evaulate start---')\n",
    "# wandb.init()\n",
    "# trainer.evaluate(eval_dataset=val_set, metric_key_prefix='val1')\n",
    "# wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습(wandb_sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {'name': f\"{MODEL_NAME}_1\",  # name : sweep_name\n",
    "                    'method': 'grid',  # 'grid', 'uniform', 'bayesian'\n",
    "                    'parameters': {\n",
    "                        'lr': {  # parameter  작성방식 여러개 있으니까, 노션 문서 참고\n",
    "                            'values': [4e-6, 5e-6, 6e-6]\n",
    "                        },\n",
    "                        'warmup_step': {\n",
    "                            \"values\": [200]\n",
    "                        },\n",
    "                        'logging_steps': {\n",
    "                            \"values\": [200]\n",
    "                        },\n",
    "                        \"batch_size\": {\n",
    "                            \"values\": [8]\n",
    "                        },\n",
    "                        \"max_epoch\": {\n",
    "                            \"values\": [2]\n",
    "                        },\n",
    "                    },\n",
    "                    # goal : maximize, minimize\n",
    "                    'metric': {'name': 'val1_accuracy', 'goal': 'maximize'}\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mode\n",
      "Create sweep with ID: tdlke131\n",
      "Sweep URL: https://wandb.ai/nlp-10/final_sentimental/sweeps/tdlke131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# print(\"Training with sweep mode\")\n",
    "\n",
    "# sweep_id = wandb.sweep(json.load(open(\"/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/sweep.json\", \"r\"), object_hook=_decode), project=\"final_sentimental\", entity=\"nlp-10\")\n",
    "# wandb.agent(sweep_id, trainWithSweep, count=int(5))\n",
    "# wandb.finish()\n",
    "from train import sweep_train\n",
    "\n",
    "print(\"Train mode\")\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"final_sentimental\", entity=\"nlp-10\")\n",
    "wandb.agent(sweep_id, sweep_train, count=3)\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='303' max='303' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [303/303 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/metrics/metrics.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  acc = load_metric('accuracy').compute(predictions=preds, references=labels)['accuracy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val1_loss': 0.5156943798065186,\n",
       " 'val1_accuracy': 0.8489475856376393,\n",
       " 'val1_f1': 0.8489475856376393,\n",
       " 'val1_runtime': 12.6519,\n",
       " 'val1_samples_per_second': 191.513,\n",
       " 'val1_steps_per_second': 23.949,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---inference start---\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '/opt/ml/input/model-roberta_large-sota_trainer'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/opt/ml/input/model-roberta_large-sota_trainer' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# model = model.to('cpu')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# MODEL_PATH = \"/opt/ml/input/model-roberta_large-sota_trainer/pytorch_model.bin\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m MODEL_PATH \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/opt/ml/input/model-roberta_large-sota_trainer\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(MODEL_PATH)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# tokenizer = RobertaTokenizer(MODEL_PATH)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# model = RobertaForSequenceClassification(MODEL_PATH)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.183.242/opt/ml/level3_nlp_finalproject-nlp-04/sentence-sentimental/train.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(MODEL_PATH)\n",
      "File \u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:693\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    690\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    691\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_candidate\u001b[39m}\u001b[39;00m\u001b[39m does not exist or is not currently imported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 693\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    695\u001b[0m \u001b[39m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[39m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1796\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1790\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1791\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1792\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1795\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1796\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1797\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1798\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1799\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1800\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1801\u001b[0m     )\n\u001b[1;32m   1803\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1804\u001b[0m     \u001b[39mif\u001b[39;00m file_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/opt/ml/input/model-roberta_large-sota_trainer'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/opt/ml/input/model-roberta_large-sota_trainer' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "print('---inference start---')\n",
    "my_text = '삼성전자 진짜 무조건 오를 듯'*20\n",
    "\n",
    "MODEL_PATH = \"/opt/ml/input/model-roberta_large-sota_trainer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "# # model = torch.load(PATH)\n",
    "model.eval()\n",
    "with torch.no_grad() :\n",
    "    temp = tokenizer(\n",
    "        my_text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        ##\n",
    "        max_length=100,\n",
    "        # stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=False\n",
    "        )\n",
    "\n",
    "    \n",
    "    temp = {\n",
    "        'input_ids':temp['input_ids'],\n",
    "        'token_type_ids':temp['token_type_ids'],\n",
    "        'attention_mask':temp['attention_mask'],\n",
    "    }\n",
    "    # print(temp)\n",
    "    \n",
    "    print(\"######################################\")\n",
    "    predicted_label = model(temp['input_ids'])\n",
    "    print(predicted_label.logits)\n",
    "    print(torch.nn.Softmax(dim=-1)(predicted_label.logits).mean(dim=0))\n",
    "\n",
    "torch.nn.Softmax(dim=-1)(predicted_label.logits).mean(dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 위에 결과에서 앞의 것이 부정 뒤에것이 긍정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정\n"
     ]
    }
   ],
   "source": [
    "result = torch.nn.Softmax(dim=-1)(predicted_label.logits).mean(dim=0)\n",
    "\n",
    "if result[0] > result[1] :\n",
    "    print(\"부정\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "981f108a204f421f158e0977940335d851edffa6dd3586828a3e1aec045160e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
